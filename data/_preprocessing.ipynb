{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.\n",
      "2.\n",
      "3.\n",
      "4.\n",
      "5.\n",
      "ðŸ“š\n",
      "BOOK NOTE // Huyen_2022_Designing Machine\n",
      "Learning Systems: an iterative process for production-\n",
      "ready applications.\n",
      "Chapter 2: Introduction to Machine Learning Systems Design\n",
      "The ultimate goal of any project within a business is, therefore, to increase\n",
      "profits, either directly or indirectly: directly such as increasing sales\n",
      "(conversion rates) and cutting costs; indirectly such as higher customer\n",
      "satisfaction and increasing time spent on a website.\n",
      "What business performance metrics is the new ML system supposed to\n",
      "influence, e.g., the amount of ads revenue, the number of monthly active\n",
      "users?\n",
      "The longer youâ€™ve adopted ML, the more efficient your pipeline will run, the\n",
      "faster your development cycle will be, the less engineering time youâ€™ll need,\n",
      "and the lower your cloud bills will be, which all lead to higher returns.\n",
      "Requirements\n",
      "Reliability: With traditional software systems, you often get a warning,\n",
      "such as a system crash or runtime error or 404. However, ML systems\n",
      "can fail silently.\n",
      "Scalability: An ML system might grow in ML model count. Autoscaling\n",
      "is an indispensable feature in many cloud services: automatically\n",
      "scaling up and down the number of machines depending on usage.\n",
      "Maintainability: Code should be documented. Code, data, and artifacts\n",
      "should be versioned. Models should be sufficiently reproducible so\n",
      "that even when the original authors are not around, other contributors\n",
      "can have sufficient contexts to build on their work.\n",
      "Adaptability: ML systems need to be able to evolve quickly.\n",
      "Iterative Process: Once a system is put into production, itâ€™ll need to be\n",
      "continually monitored and updated. The process looks more like a\n",
      "cycle with a lot of back and forth between different steps.\n",
      "An ML problem is defined by inputs, outputs, and the objective function that\n",
      "guides the learning process.\n",
      "Classification models classify inputs into different categories.\n",
      "Regression models output a continuous value.\n",
      "A regression model can easily be framed as a classification model and vice\n",
      "versa.\n",
      "Within classification problems, the fewer classes there are to classify, the\n",
      "simpler the problem is. The simplest is binary classification, where there are\n",
      "only two possible classes.\n",
      "When there are more than two classes, the problem becomes multiclass\n",
      "classification.\n",
      "When the number of classes is high, we say the classification task has high\n",
      "cardinality.\n",
      "ML models typically need at least 100 examples for each class to learn to\n",
      "classify that class. So if you have 1,000 classes, you already need at least\n",
      "100,000 examples.\n",
      "Solution: When the number of classes is large, hierarchical classification might\n",
      "be useful. In hierarchical classification, you have a classifier to first classify\n",
      "each example into one of the large groups. Then you have another classifier to\n",
      "classify this example into one of the subgroups.\n",
      "In both binary and multiclass classification, each example belongs to exactly\n",
      "one class. When an example can belong to multiple classes, we have a\n",
      "multilabel classification problem.\n",
      "To learn, an ML model needs an objective function to guide the learning\n",
      "process. An objective function is also called a loss function because the\n",
      "objective of the learning process is usually to minimize (or optimize) the loss\n",
      "caused by wrong predictions.\n",
      "Progress in the last decade shows that the success of an ML system depends\n",
      "largely on the data it was trained on. Instead of focusing on improving ML\n",
      "algorithms, most companies focus on managing and improving their data.\n",
      "Mind might be disguised as inductive biases or intelligent architectural designs.\n",
      "Data might be grouped together with computation since more data tends to\n",
      "require more computation.\n",
      "Judea Pearl (The Book of Why): â€œData is profoundly dumb.â€\n",
      "â€œML will not be the same in 3â€“5 years, and ML folks who continue to follow the\n",
      "current data-centric paradigm will find themselves outdated, if not jobless.\n",
      "Take note.â€\n",
      "Peter Norvig (Google director of search quality): â€œWe donâ€™t have better\n",
      "algorithms. We just have more data.â€\n",
      "Every project must start with why this project needs to happen, and ML\n",
      "projects are no exception.\n",
      "Chapter 3: Data Engineering Fundamentals\n",
      "If data models describe the data in the real world, databases specify how the\n",
      "data should be stored on machines.\n",
      "Data Sources\n",
      "Understanding the sources your data comes from can help you use your data\n",
      "more efficiently.\n",
      "One source is user input data, data explicitly input by users.\n",
      "User input data can be easily malformatted.\n",
      "System-generated data is the data generated by different components of your\n",
      "systems, which include various types of logs and system outputs such as\n",
      "model predictions.\n",
      "Because debugging ML systems is hard, itâ€™s a common practice to log\n",
      "everything you can.\n",
      "To analyze logs: Logstash, Datadog, Logz.io, etc. Many of them use ML models\n",
      "to help you process and make sense of your massive number of logs.\n",
      "There are also internal databases, generated by various services and enterprise\n",
      "applications in a company. These databases manage their assets such as\n",
      "inventory, customer relationships, users, and more.\n",
      "First-party data is the data that your company already collects about your users\n",
      "or customers. Second-party data is the data collected by another company on\n",
      "their own customers that they make available to you, though youâ€™ll probably\n",
      "have to pay for it. Third-party data companies collect data on the public who\n",
      "arenâ€™t their direct customers.\n",
      "Data Formats\n",
      "â— How do I store multimodal data, e.g., a sample that might contain both\n",
      "images and texts?\n",
      "â— Where do I store my data so that itâ€™s cheap and still fast to access?\n",
      "â— How do I store complex models so that they can be loaded and run\n",
      "correctly on different hardware?\n",
      "The process of converting a data structure or object state into a format that\n",
      "can be stored or transmitted and reconstructed later is data serialization.\n",
      "1. JSON = JavaScript Object Notation: Its key-value pair paradigm is\n",
      "simple but powerful, capable of handling data of different levels of\n",
      "1.\n",
      "structuredness.\n",
      "2. Row-Major Versus Column-Major Format: CSV (comma-separated\n",
      "values) is row-major, which means consecutive elements in a row are\n",
      "stored next to each other in memory. Parquet is column-major, which\n",
      "means consecutive elements in a column are stored next to each\n",
      "other.\n",
      "â€”> Because modern computers process sequential data more\n",
      "efficiently than nonsequential data, if a table is row-major, accessing\n",
      "its rows will be faster than accessing its columns in expectation. This\n",
      "means that for row-major formats, accessing data by rows is expected\n",
      "to be faster than accessing data by columns.\n",
      "3. NumPy Versus pandas\n",
      "- pandas is built around DataFrame, a concept inspired by Râ€™s Data\n",
      "Frame, which is column-major. A DataFrame is a two-dimensional table\n",
      "with rows and columns.\n",
      "- In NumPy, the major order can be specified. When an ndarray is\n",
      "created, itâ€™s row-major by default if you donâ€™t specify the order.\n",
      "â€”> Accessing a DataFrame by row is so much slower than accessing\n",
      "the same DataFrame by column.\n",
      "4. Text Versus Binary Format: Text files are files that are in plain text,\n",
      "which usually means they are human-readable. Binary files are the\n",
      "catchall that refers to all nontext files. As the name suggests, binary\n",
      "files are typically files that contain only 0s and 1s, and are meant to be\n",
      "read or used by programs that know how to interpret the raw bytes.\n",
      "â€”> You want to store the number 1000000. If you store it in a text file,\n",
      "itâ€™ll require 7 characters, and if each character is 1 byte, itâ€™ll require 7\n",
      "bytes. If you store it in a binary file as int32, itâ€™ll take only 32 bits or 4\n",
      "bytes.\n",
      "=> AWS recommends using the Parquet format because â€œthe Parquet\n",
      "format is up to 2x faster to unload and consumes up to 6x less storage\n",
      "in Amazon S3, compared to text formats.â€\n",
      "Data Models\n",
      "Data models describe how data is represented.\n",
      "How you choose to represent data not only affects the way your systems are\n",
      "built but also the problems your systems can solve.\n",
      "Relational Models\n",
      "Data is organized into relations; each relation is a set of tuples. A table is an\n",
      "accepted visual representation of a relation, and each row of a table makes up\n",
      "a tuple.\n",
      "One major downside of normalization is that your data is now spread across\n",
      "multiple relations. You can join the data from different relations back together,\n",
      "but joining can be expensive for large tables.\n",
      "Databases built around the relational data model are relational databases.\n",
      "The language that you can use to specify the data that you want from a\n",
      "database is called a query language. The most popular query language for\n",
      "relational databases today is SQL.\n",
      "SQL = declarative language\n",
      "imperative paradigm = you specify the steps needed for an action and the\n",
      "computer executes these steps to return the outputs.\n",
      "declarative paradigm = you specify the outputs you want, and the computer\n",
      "figures out the steps needed to get you the queried outputs.\n",
      "SQL can be Turing-complete, which means that, in theory, SQL can be used to\n",
      "solve any computation problem (without making any guarantee about the time\n",
      "or memory required). (https://wiki.postgresql.org/wiki/Cyclic_Tag_System)\n",
      "Declarative ML Systems:\n",
      "https://docs.h2o.ai/h2o/latest-stable/h2o-docs/automl.html\n",
      "https://github.com/ludwig-ai/ludwig\n",
      "NoSQL\n",
      "Two major types of nonrelational models are the document model and the\n",
      "graph model. The document model targets use cases where data comes in self-\n",
      "contained documents and relationships between one document and another\n",
      "are rare. The graph model goes in the opposite direction, targeting use cases\n",
      "where relationships between data items are common and important.\n",
      "Compared to the relational model, itâ€™s harder and less efficient to execute joins\n",
      "across documents compared to across tables.\n",
      "Structured Versus Unstructured Data\n",
      "Structured data follows a predefined data model, also known as a data schema.\n",
      "The disadvantage of structured data is that you have to commit your data to a\n",
      "predefined schema. If your schema changes, youâ€™ll have to retrospectively\n",
      "update all your data, often causing mysterious bugs in the process.\n",
      "Unstructured data doesnâ€™t adhere to a predefined data schema. Itâ€™s usually text\n",
      "but can also be numbers, dates, images, audio, etc.\n",
      "Unstructured data also allows for more flexible storage options.\n",
      "A repository for storing structured data is called a data warehouse.\n",
      "Data warehouses are used to store data that has been processed into formats\n",
      "ready to be used.\n",
      "A repository for storing unstructured data is called a data lake.\n",
      "Data lakes are usually used to store raw data before processing.\n",
      "Data Storage Engines and Processing\n",
      "Data formats and data models specify the interface for how users can store and\n",
      "retrieve data. Storage engines, also known as databases, are the\n",
      "implementation of how data is stored and retrieved on machines.\n",
      "Transactional and Analytical Processing\n",
      "The transactions are inserted as they are generated, and occasionally updated\n",
      "when something changes, or deleted when they are no longer needed (= online\n",
      "transaction processing (OLTP).\n",
      "Transactional databases are designed to process online transactions and\n",
      "satisfy the low latency, high availability requirements.\n",
      "Analytical questions require aggregating data in columns across multiple rows\n",
      "of data. Analytical databases are designed for this purpose (= online analytical\n",
      "processing (OLAP).\n",
      "ETL: Extract, Transform, and Load\n",
      "When data is extracted from different sources, itâ€™s first transformed into the\n",
      "desired format before being loaded into the target destination such as a\n",
      "database or a data warehouse.\n",
      "In the extracting phase, you need to validate your data and reject the data that\n",
      "doesnâ€™t meet your requirements.\n",
      "Transform is, where most of the data processing is done. You might want to join\n",
      "data from multiple sources and clean it. You might want to standardize the\n",
      "value ranges.\n",
      "Load is deciding how and how often to load your transformed data into the\n",
      "target destination, which can be a file, a database, or a data warehouse.\n",
      "Finding it difficult to keep data structured, some companies had this idea: â€œWhy\n",
      "not just store all data in a data lake so we donâ€™t have to deal with schema\n",
      "changes?\n",
      "This process of loading data into storage first and then processing it later is\n",
      "sometimes called ELT (extract, load, transform).\n",
      "BUT: Itâ€™s inefficient to search through a massive amount of raw data for the\n",
      "data that you want.\n",
      "BUT 2: As companies switch to running applications on the cloud and\n",
      "infrastructures become standardized, data structures also become\n",
      "standardized.\n",
      "Modes of Dataflow\n",
      "How do we pass data between different processes that donâ€™t share memory?\n",
      "When data is passed from one process to another, we say that the data flows\n",
      "from one process to another, which gives us a dataflow.\n",
      "â— Data passing through databases\n",
      "â— Data passing through services using requests such as the requests\n",
      "provided by REST and RPC APIs (e.g., POST/GET requests)\n",
      "â—‹ Because processes communicate through requests, we say that\n",
      "this is request-driven.\n",
      "â—‹ The most popular styles of requests used for passing data through\n",
      "networks are REST (representational state transfer) and RPC\n",
      "(remote procedure call)\n",
      "â—‹ Implementations of a REST architecture are said to be RESTful.\n",
      "â—‹ REST doesnâ€™t exactly mean HTTP because HTTP is just an\n",
      "implementation of REST.\n",
      "â— Data passing through a real-time transport like Apache Kafka and\n",
      "Amazon Kinesis\n",
      "â—‹ each service can write data to a database and other services that\n",
      "need the data can read from that database.\n",
      "â—‹ A piece of data broadcast to a real-time transport is called an\n",
      "event. This architecture is, therefore, also called event-driven.\n",
      "â—‹ pubsub (publish-subscribe) model = any service can publish to\n",
      "different topics in a real-time transport, and any service that\n",
      "subscribes to a topic can read all the events in that topic. The\n",
      "services that produce data donâ€™t care about what services\n",
      "consume their data.\n",
      "Batch Processing Versus Stream Processing\n",
      "Once your data arrives in data storage engines like databases, data lakes, or\n",
      "data warehouses, it becomes historical data.\n",
      "Historical data is often processed in batch jobsâ€”jobs that are kicked off\n",
      "periodically.\n",
      "When data is processed in batch jobs, we refer to it as batch processing.\n",
      "When you have data in real-time transports like Apache Kafka and Amazon\n",
      "Kinesis, we say that you have streaming data.\n",
      "Stream processing refers to doing computation on streaming data.\n",
      "Stream processing, when done right, can give low latency because you can\n",
      "process data as soon as data is generated, without having to first write it into\n",
      "databases.\n",
      "Batch featuresâ€”features extracted through batch processingâ€”are also known\n",
      "as static features.\n",
      "Streaming featuresâ€” features extracted through stream processingâ€”are also\n",
      "known as dynamic features.\n",
      "Chapter 4: Training Data\n",
      "Data is full of potential biases. These biases have many possible causes. There\n",
      "are biases caused during collecting, sampling, or labeling. Historical data might\n",
      "be embedded with human biases, and ML models, trained on this data, can\n",
      "perpetuate them. Use data but donâ€™t trust it too much!\n",
      "Sampling\n",
      "Sampling happens in many steps of an ML project lifecycle, such as sampling\n",
      "from all possible real-world data to create training data; sampling from a given\n",
      "dataset to create splits for training, validation, and testing, etc.\n",
      "Nonprobability Sampling\n",
      "Nonprobability sampling is when the selection of data isnâ€™t based on any\n",
      "probability criteria.\n",
      "Beispiel: Convenience sampling\n",
      "â€“ Samples of data are selected based on their availability. This sampling\n",
      "method is popular because, well, itâ€™s convenient.\n",
      "The samples selected by nonprobability criteria are not representative of the\n",
      "real-world data and therefore are riddled with selection biases.\n",
      "Language models are often trained not with data that is representative of all\n",
      "possible texts but with data that can be easily collectedâ€”Wikipedia, Common\n",
      "Crawl, Reddit.\n",
      "Simple Random Sampling\n",
      "In the simplest form of random sampling, you give all samples in the population\n",
      "equal probabilities of being selected. For example, you randomly select 10% of\n",
      "the population, giving all members of this population an equal 10% chance of\n",
      "being selected.\n",
      "Stratified Sampling\n",
      "To avoid the drawback of simple random sampling, you can first divide your\n",
      "population into the groups that you care about and sample from each group\n",
      "separately.\n",
      "Weighted Sampling\n",
      "Each sample is given a weight, which determines the probability of it being\n",
      "selected.\n",
      "This method allows you to leverage domain expertise.\n",
      "Reservoir Sampling\n",
      "One solution for this problem is reservoir sampling. The algorithm involves a\n",
      "reservoir, which can be an array.\n",
      "Importance Sampling\n",
      "It allows us to sample from a distribution when we only have access to another\n",
      "distribution.\n",
      "Labeling\n",
      "The performance of an ML model still depends heavily on the quality and\n",
      "quantity of the labeled data itâ€™s trained on.\n",
      "Hand Labels\n",
      "Hand-labeling data can be expensive, especially if subject matter expertise is\n",
      "required.\n",
      "Hand labeling poses a threat to data privacy\n",
      "Hand labeling is slow\n",
      "Slow labeling leads to slow iteration speed and makes your model less adaptive\n",
      "to changing environments and requirements. If the task changes or data\n",
      "changes, youâ€™ll have to wait for your data to be relabeled before updating your\n",
      "model.\n",
      "Label multiplicity\n",
      "To obtain enough labeled data, companies have to use data from multiple\n",
      "sources and rely on multiple annotators who have different levels of expertise.\n",
      "These different data sources and annotators also have different levels of\n",
      "accuracy.\n",
      "Data lineage\n",
      "Itâ€™s good practice to keep track of the origin of each of your data samples as\n",
      "well as its labels, a technique known as data lineage.\n",
      "Data lineage helps you both flag potential biases in your data and debug your\n",
      "models.\n",
      "Natural Labels\n",
      "Tasks with natural labels are tasks where the modelâ€™s predictions can be\n",
      "automatically evaluated or partially evaluated by the system.\n",
      "Semi-supervision\n",
      "A classic semi-supervision method is self-training. You start by training a\n",
      "model on your existing set of labeled data and use this model to make\n",
      "predictions for unlabeled samples. Assuming that predictions with high raw\n",
      "probability scores are correct, you add the labels predicted with high\n",
      "probability to your training set and train a new model on this expanded training\n",
      "set. This goes on until youâ€™re happy with your model performance.\n",
      "Semi-supervision is the most useful when the number of training labels is\n",
      "limited.\n",
      "Transfer learning\n",
      "Transfer learning refers to the family of methods where a model developed for a\n",
      "task is reused as the starting point for a model on a second task.\n",
      "Transfer learning is especially appealing for tasks that donâ€™t have a lot of\n",
      "labeled data.\n",
      "Active learning\n",
      "Active learning is a method for improving the efficiency of data labels.\n",
      "Instead of randomly labeling data samples, you label the samples that are most\n",
      "help- ful to your models according to some metrics or heuristics.\n",
      "Class Imbalance\n",
      "Class imbalance typically refers to a problem in classification tasks where there\n",
      "is a substantial difference in the number of samples in each class of the training\n",
      "data.\n",
      "ML, especially deep learning, works well in situations when the data distribution\n",
      "is more balanced, and usually not so well when the classes are heavily\n",
      "imbalanced\n",
      "Precision, Recall, and F1\n",
      "true positive rate (also known as recall)\n",
      "false positive rate (also known as the probability of false alarm)\n",
      "We can plot the true positive rate against the false positive rate for different\n",
      "thresholds. This plot is known as the ROC curve (receiver operating\n",
      "characteristics)\n",
      "â€”> When your model is perfect, the recall is 1.0, and the curve is just a line at\n",
      "the top\n",
      "The area under the curve (AUC) measures the area under the ROC curve. Since\n",
      "the closer to the perfect line the better, the larger this area the better\n",
      "Data-level methods: Resampling\n",
      "Data-level methods modify the distribution of the training data to reduce the\n",
      "level of imbalance to make it easier for the model to learn. A common family of\n",
      "techniques is resampling. Resampling includes oversampling, adding more\n",
      "instances from the minority classes, and undersampling, removing instances of\n",
      "the majority classes.\n",
      "When you resample your training data, never evaluate your model on resampled\n",
      "data, since it will cause your model to overfit to that resampled distribution.\n",
      "Undersampling runs the risk of losing important data from removing data.\n",
      "Data Augmentation\n",
      "Data augmentation is a family of techniques that are used to increase the\n",
      "amount of training data.\n",
      "In computer vision, the simplest data augmentation technique is to randomly\n",
      "modify an image while preserving its label.\n",
      "In NLP, you can randomly replace a word with a similar word, assuming that this\n",
      "replacement wouldnâ€™t change the meaning or the sentiment of the sentence.\n",
      "Perturbation (StÃ¶rung)\n",
      "Perturbation is also a label-preserving operation\n",
      "Using deceptive data to trick a neural network into making wrong predictions is\n",
      "called adversarial attacks.\n",
      "Adding noise to samples is a common technique to create adversarial samples.\n",
      "Data Synthesis\n",
      "Since collecting data is expensive and slow, with many potential privacy\n",
      "concerns, itâ€™d be a dream if we could sidestep it altogether and train our\n",
      "models with synthesized data. Even though weâ€™re still far from being able to\n",
      "synthesize all training data, itâ€™s possible to synthesize some training data to\n",
      "boost a modelâ€™s performance.\n",
      "Chapter 5: Feature Engineering\n",
      "State-of-the-art model architectures can still perform poorly if they donâ€™t use a\n",
      "good set of features.\n",
      "A large part of many ML engineering and data science jobs is to come up with\n",
      "new useful features.\n",
      "Learned Features Versus Engineered Features\n",
      "The promise of deep learning is that we wonâ€™t have to handcraft features. For\n",
      "this reason, deep learning is sometimes called feature learning.\n",
      "Many features can be automatically learned and extracted by algorithms.\n",
      "However, weâ€™re still far from the point where all features can be automated.\n",
      "EXKURS n-gram: an n-gram is a contiguous sequence of n items from a given\n",
      "sample of text.\n",
      "The items can be phonemes, syllables, letters, or words. For example, given the\n",
      "post â€œI like food,â€ its word-level 1-grams are [â€œIâ€, â€œlikeâ€, â€œfoodâ€] and its word-\n",
      "level 2-grams are [â€œI likeâ€, â€œlike foodâ€].\n",
      "Once youâ€™ve generated n-grams for your training data, you can create a\n",
      "vocabulary that maps each n-gram to an index. Then you can convert each post\n",
      "into a vector based on its n-gramsâ€™ indices.\n",
      "Feature engineering requires knowledge of domain-specific techniquesâ€”in this\n",
      "case, the domain is natural language processing (NLP) and the native language\n",
      "of the text.\n",
      "With Deep Learning: Instead of having to worry about lemmatization,\n",
      "punctuation, or stopword removal, you can just split your raw text into words\n",
      "(i.e., tokenization), create a vocabulary out of those words, and convert each of\n",
      "your words into one-shot vectors using this vocabulary.\n",
      "The process of choosing what information to use and how to extract this\n",
      "information into a format usable by your ML models is feature engineering.\n",
      "For complex tasks such as recommending videos for users to watch next on\n",
      "TikTok, the number of features used can go up to millions.\n",
      "For domain-specific tasks, you might need subject matter expertise.\n",
      "Common Feature Engineering Operations\n",
      "Handling Missing Values\n",
      "Missing not at random (MNAR)\n",
      "This is when the reason a value is missing is because of the true value itself.\n",
      "Missing at random (MAR)\n",
      "This is when the reason a value is missing is not due to the value itself, but due\n",
      "to another observed variable.\n",
      "Missing completely at random (MCAR)\n",
      "This is when thereâ€™s no pattern in when the value is missing.\n",
      "This type of missing is very rare. There are usually reasons why certain values\n",
      "are missing, and you should investigate.\n",
      "Solution:\n",
      "One way to delete is column deletion: if a variable has too many missing values,\n",
      "just remove that variable.\n",
      "Another way to delete is row deletion: if a sample has missing value(s), just\n",
      "remove that sample.\n",
      "However, removing rows of data can also remove important information that\n",
      "your model needs to make predictions (especially when MNAR)\n",
      "On top of that, removing rows of data can create biases in your model\n",
      "(especially when MAR)\n",
      "If you donâ€™t want to delete missing values, you will have to impute them, which\n",
      "means â€œfill them with certain values.â€\n",
      "One common practice is to fill in missing values with their defaults (or with\n",
      "median, mean or mode - most common value).\n",
      "You want to avoid filling missing values with possible values.\n",
      "Multiple techniques might be used at the same time or in sequence to handle\n",
      "missing values for a particular set of data.\n",
      "â€”> There is no perfect way to handle missing values\n",
      "Scaling\n",
      "Before inputting features into models, itâ€™s important to scale them to similar\n",
      "ranges. This process is called feature scaling.\n",
      "This is one of the simplest things you can do that often results in a\n",
      "performance boost for your model.\n",
      "An intuitive way to scale your features is to get them to be in the range [0, 1].\n",
      "If you think that your variables might follow a normal distribution, it might be\n",
      "helpful to normalize them so that they have zero mean and unit variance\n",
      "(standardization).\n",
      "In many cases, the log transformation can help reduce the skewness of your\n",
      "data.\n",
      "Encoding Categorical Features\n",
      "In production, categories change.\n",
      "One solution to this problem (dynamic change in categories) is the hashing\n",
      "trick.\n",
      "The gist of this trick is that you use a hash function to generate a hashed value\n",
      "of each category. The hashed value will become the index of that category.\n",
      "Because you can specify the hash space, you can fix the number of encoded\n",
      "values for a feature in advance, without having to know how many categories\n",
      "there will be.\n",
      "One problem with hashed functions is collision: two categories being assigned\n",
      "the same index.\n",
      "You can choose a hash space large enough to reduce the collision.\n",
      "Feature Crossing\n",
      "Feature crossing is the technique to combine two or more features to generate\n",
      "new features.\n",
      "This technique is useful to model the nonlinear relationships between features.\n",
      "Itâ€™s less important in neural networks, but it can still be useful because explicit\n",
      "feature crossing occasionally helps neural networks learn nonlinear\n",
      "relationships faster.\n",
      "Discrete and Continuous Positional Embeddings\n",
      "Consider the task of language modeling where you want to predict the next\n",
      "token (e.g., a word, character, or subword) based on the previous sequence of\n",
      "tokens.\n",
      "EXKURS embeddings: An embedding is a vector that represents a piece of\n",
      "data. We call the set of all possible embeddings generated by the same\n",
      "algorithm for a type of data â€œan embedding space.â€ All embedding vectors in\n",
      "the same space are of the same size.\n",
      "The embedding size for positions is usually the same as the embedding size for\n",
      "words so that they can be summed.\n",
      "Because the embeddings change as the model weights get updated, we say\n",
      "that the position embeddings are learned.\n",
      "The embedding for each position is still a vector with S elements (S is the\n",
      "position embedding size), but each element is predefined using a function,\n",
      "usually sine and cosine. In the original Transformer paper, if the element is at an\n",
      "even index, use sine. Else, use cosine.\n",
      "Fixed positional embedding is a special case of what is known as Fourier\n",
      "features. If positions in positional embeddings are discrete, Fourier features\n",
      "can also be continuous.\n",
      "Data Leakage\n",
      "Data leakage refers to the phenomenon when a form of the label â€œleaksâ€ into\n",
      "the set of features used for making predictions, and this same information is\n",
      "not available during inference.\n",
      "Itâ€™s dangerous because it can cause your models to fail in an unexpected and\n",
      "spectacular way\n",
      "Splitting time-correlated data randomly instead of by time\n",
      "In many cases, data is time-correlated, which means that the time the data is\n",
      "generated affects its label distribution.\n",
      "To prevent future information from leaking into the training process and\n",
      "allowing models to cheat during evaluation, split your data by time, instead of\n",
      "splitting randomly, whenever possible.\n",
      "For example, if you have data from five weeks, use the first four weeks for the\n",
      "train split, then randomly split week 5 into validation and test.\n",
      "Scaling before splitting\n",
      "One common mistake is to use the entire training data to generate global\n",
      "statistics before splitting it into different splits, leaking the mean and variance\n",
      "of the test samples into the training process, and allowing a model to adjust its\n",
      "predictions for the test samples.\n",
      "To avoid this type of leakage, always split your data first before scaling, then\n",
      "use the statistics from the train split to scale all the splits.\n",
      "Filling in missing data with statistics from the test split\n",
      "Leakage might occur if the mean or median is calculated using the entire data\n",
      "instead of just the train split.\n",
      "It can be prevented by using only statistics from the train split to fill in missing\n",
      "values in all the splits.\n",
      "Poor handling of data duplication before splitting\n",
      "If you have duplicates or near-duplicates in your data, failing to remove them\n",
      "before splitting your data might cause the same samples to appear in both train\n",
      "and validation/test splits.\n",
      "Data duplication can result from data collection or merging of different data\n",
      "sources.\n",
      "To avoid this, always check for duplicates before splitting and also after\n",
      "splitting just to make sure. If you oversample your data, do it after splitting.\n",
      "Group leakage\n",
      "A group of examples have strongly correlated labels but are divided into\n",
      "different splits.\n",
      "his type of leakage is common for objective detection tasks that contain photos\n",
      "of the same object taken milliseconds apartâ€”some of them landed in the train\n",
      "split while others landed in the test split.\n",
      "Itâ€™s hard avoiding this type of data leakage without understanding how your\n",
      "data was generated.\n",
      "Leakage from data generation process\n",
      "Detecting this type of data leakage requires a deep understanding of the way\n",
      "data is collected.\n",
      "Thereâ€™s no foolproof way to avoid this type of leakage, but you can mitigate the\n",
      "risk by keeping track of the sources of your data and understanding how it is\n",
      "collected and processed.\n",
      "Normalize your data so that data from different sources can have the same\n",
      "means and variances.\n",
      "Detecting Data Leakage\n",
      "Measure the predictive power of each feature or a set of features with respect\n",
      "to the target variable (label).\n",
      "If a feature has an unusually high correlation, investigate how this feature is\n",
      "generated and whether the correlation makes sense.\n",
      "Do ablation studies to measure how important a feature or a set of features is\n",
      "to your model.\n",
      "If removing a feature causes the modelâ€™s performance to deteriorate\n",
      "significantly, investigate why that feature is so important.\n",
      "Keep an eye out for new features added to your model. If adding a new feature\n",
      "significantly improves your modelâ€™s performance, either that feature is really\n",
      "good or that feature just contains leaked information about labels.\n",
      "Be very careful every time you look at the test split.\n",
      "If you use the test split in any way other than to report a modelâ€™s final\n",
      "performance, you risk leaking information from the future into your training\n",
      "process.\n",
      "Engineering Good Features\n",
      "Generally, adding more features leads to better model performance.\n",
      "If a feature doesnâ€™t help a model make good predictions, regularization\n",
      "techniques like L1 regularization should reduce that featureâ€™s weight to 0.\n",
      "Feature Importance\n",
      "If you use a classical ML algorithm like boosted gradient trees, the easiest way\n",
      "to measure the importance of your features is to use built-in feature importance\n",
      "functions implemented by XGBoost.\n",
      "For more model-agnostic methods, you might want to look into SHAP (SHapley\n",
      "Additive exPlanations).\n",
      "SHAP is great because it not only measures a featureâ€™s importance to an entire\n",
      "model, but it also measures each featureâ€™s contribution to a modelâ€™s specific\n",
      "prediction.\n",
      "InterpretML: https://github.com/interpretml/interpret\n",
      "Often, a small number of features accounts for a large portion of your modelâ€™s\n",
      "feature importance.\n",
      "Feature Generalization\n",
      "Not all features generalize equally.\n",
      "Measuring feature generalization is a lot less scientific than measuring feature\n",
      "importance, and it requires both intuition and subject matter expertise on top of\n",
      "statistical knowledge.\n",
      "Coverage is the percentage of the samples that has values for this feature in\n",
      "the dataâ€” so the fewer values that are missing, the higher the coverage.\n",
      "feature distribution: If the set of values that appears in the seen data (such as\n",
      "the train split) has no overlap with the set of values that appears in the unseen\n",
      "data (such as the test split), this feature might even hurt your modelâ€™s\n",
      "performance\n",
      "Chapter 6: Model development and Offline Evaluation\n",
      "It allows to play around with different algorithms and techniques, even the\n",
      "latest ones.\n",
      "To build an ML model, we first need to select the ML model to build.\n",
      "Model development is an iterative process. After each iteration, youâ€™ll want to\n",
      "com- pare your modelâ€™s performance against its performance in previous\n",
      "iterations and evaluate how suitable this iteration is for production.\n",
      "Model Evaluation\n",
      "If you had unlimited time and compute power, the rational thing to do would be\n",
      "to try all possible solutions and see what is best for you. However, time and\n",
      "compute power are limited resources, and you have to be strategic about what\n",
      "models you select.\n",
      "However, even though deep learning is finding more use cases in production,\n",
      "classical ML algorithms are not going away. Many recommender systems still\n",
      "rely on collaborative filtering and matrix factorization. Tree-based algorithms,\n",
      "including gradient-boosted trees, still power many classifica- tion tasks with\n",
      "strict latency requirements.\n",
      "Example: A k-means clustering model might be used to extract features to input\n",
      "into a neural network.\n",
      "When selecting a model for your problem, you donâ€™t choose from every\n",
      "possible model out there, but usually focus on a set of models suitable for your\n",
      "problem.\n",
      "Example: If your boss tells you to build a system to detect toxic tweets, you\n",
      "know that this is a text classification problemâ€”given a piece of text, classify\n",
      "whether itâ€™s toxic or notâ€”and common models for text classification include\n",
      "naive Bayes, logistic regression, recurrent neural networks, and transformer-\n",
      "based models such as BERT, GPT, and their variants.\n",
      "Knowledge of common ML tasks and the typical approaches to solve them is\n",
      "essential in this process.\n",
      "Different types of algorithms require different numbers of labels as well as\n",
      "different amounts of compute power.\n",
      "When considering what model to use, itâ€™s important to consider not only the\n",
      "modelâ€™s performance, measured by metrics such as accuracy, F1 score, and log\n",
      "loss, but also its other properties, such as how much data, compute, and time it\n",
      "needs to train, whatâ€™s its inference latency, and interpretability.\n",
      "To understand different algorithms, the best way is to equip yourself with basic\n",
      "ML knowledge and run experiments with the algorithms youâ€™re interested in.\n",
      "Important: Follow major ML conferences such as NeurIPS, ICLR, and ICML, as\n",
      "well as following researchers whose work has a high signal-to-noise ratio on\n",
      "Twitter.\n",
      "Six tips for model selection\n",
      "1. Avoid the state-of-the-art trap\n",
      "â€”> What means â€žstate-of-the-artâ€œ?\n",
      "Being state-of-the-art often means that it performs better than\n",
      "existing models on some static datasets. It doesnâ€™t mean that this\n",
      "model will be fast enough or cheap enough for you to implement.\n",
      "If thereâ€™s a solution that can solve your problem that is much cheaper\n",
      "and simpler than state-of-the-art models, use the simpler solution.\n",
      "2. Start with the simplest models\n",
      "Zen of Python states that â€œsimple is better than complex,â€\n",
      "1. simpler models are easier to deploy\n",
      "2. easier to understand your model\n",
      "3. simplest model serves as a baseline\n",
      "â€”> pretrained BERT models are complex, but they require little effort\n",
      "to get started with, especially if you use a ready-made implementation\n",
      "like the one in Hugging Faceâ€™s Transformer.\n",
      "3. Avoid human biases in selecting models\n",
      "Part of the process of evaluating an ML architecture is to experiment\n",
      "with different features and different sets of hyperparameters to find\n",
      "the best model of that architecture.\n",
      "If an engineer is more excited about an architecture, they will likely\n",
      "spend a lot more time experimenting with it, which might result in\n",
      "better-performing models for that architecture.\n",
      "When comparing different architectures, itâ€™s important to compare\n",
      "them under comparable setups.\n",
      "4. Evaluate good performance now versus good performance later\n",
      "A simple way to estimate how your modelâ€™s performance might change\n",
      "with more data is to use learning curves\n",
      "Learning Curve: a plot of its performanceâ€”e.g., training loss, training\n",
      "accuracy, validation accuracyâ€”against the number of training samples\n",
      "it uses\n",
      "While evaluating models, you might want to take into account their\n",
      "potential for improvements in the near future, and how easy/difficult it\n",
      "is to achieve those improvements.\n",
      "5. Evaluate trade-offs\n",
      "A more complex model can give a better performance, but its results\n",
      "5.\n",
      "are less interpretable.\n",
      "6. Understand your modelâ€™s assumptions\n",
      "The real world is intractably complex, and models can only\n",
      "approximate using assumptions.\n",
      "Ensembles\n",
      "One method that has consistently given a performance boost is to use an\n",
      "ensemble of multiple models instead of just an individual model to make\n",
      "predictions.\n",
      "Each model in the ensemble is called a base learner.\n",
      "Ensembling methods are less favored in production because ensembles are\n",
      "more complex to deploy and harder to maintain.\n",
      "When creating an ensemble, the less correlation there is among base learners,\n",
      "the better the ensemble will be.\n",
      "Therefore, itâ€™s common to choose very different types of models for an\n",
      "ensemble.\n",
      "1. Bagging = bootstrap aggregating\n",
      "â€“ designed to improve both the training stability and accuracy of ML\n",
      "algorithms.\n",
      "â€“ reduces variance and helps to avoid overfitting.\n",
      "â€“ Given a dataset, instead of training one classifier on the entire dataset,\n",
      "you sample with replacement to create different datasets, called\n",
      "bootstraps, and train a classification or regression model on each of\n",
      "these bootstraps.\n",
      "â€“ Why Sampling? Sampling with replacement ensures that each\n",
      "bootstrap is created independently from its peers.\n",
      "1. Boosting = a family of iterative ensemble algorithms that convert weak\n",
      "learners to strong ones\n",
      "â€“ Each learner in this ensemble is trained on the same set of samples,\n",
      "but the samples are weighted differently among iterations.\n",
      "â€“ future weak learners focus more on the examples that previous weak\n",
      "learners misclassified.\n",
      "â€“ XGBoost, a variant of GBM, used to be the algorithm of choice for\n",
      "many winning teams in ML competitions.\n",
      "1. Stacking\n",
      "â€“ you train base learners from the training data and then create a meta-\n",
      "learner that combines the outputs of the base learners to output final\n",
      "predictions\n",
      "â€“ meta-learner can be as simple as a heuristic: you take the majority\n",
      "â€“\n",
      "vote (for classification tasks) or the average vote (for regression tasks)\n",
      "from all base learners.\n",
      "Experiment Tracking and Versioning\n",
      "Itâ€™s important to keep track of all the definitions needed to re-create an\n",
      "experiment and its relevant artifacts.\n",
      "An artifact is a file generated during an experimentâ€”examples of artifacts can\n",
      "be files that show the loss curve, evaluation loss graph, etc.\n",
      "The process of tracking the progress and results of an experiment is called\n",
      "experiment tracking.\n",
      "The process of logging all the details of an experiment for the purpose of\n",
      "possibly recreating it later or comparing it with other experiments is called\n",
      "versioning.\n",
      "Experiment tracking\n",
      "= babysitting the learning processes\n",
      "Itâ€™s important to track whatâ€™s going on during training not only to detect and\n",
      "address these issues but also to evaluate whether your model is learning\n",
      "anything useful.\n",
      "What should/could be tracked:\n",
      "Loss curve, model performance metrics, corresponding sample, prediction,\n",
      "ground truth label, speed of model, system performance metrics, parameter,\n",
      "hyperparameter\n",
      "A simple way to track your experiments is to automatically make copies of all\n",
      "the code files needed for an experiment and log all outputs with their\n",
      "timestamps.\n",
      "Versioning\n",
      "ML systems are part code, part data, so you need to not only version your code\n",
      "but your data as well.\n",
      "Code versioning tools allow users to revert to a previous version of the\n",
      "codebase by keeping copies of all the old files.\n",
      "Debugging ML Models\n",
      "ML models fail silently\n",
      "even when you think youâ€™ve found the bug, it can be frustratingly slow to\n",
      "validate whether the bug has been fixed\n",
      "debugging ML models is hard because of their cross-functional complexity.\n",
      "There are many components in an ML system: data, labels, features, ML\n",
      "algorithms, code, infrastructure, etc. These different components might be\n",
      "owned by different teams.\n",
      "some of the things that might cause an ML model to fail:\n",
      "â€“ Theoretical constraints\n",
      "â€“ Poor implementation of model\n",
      "â€“ Poor choice of hyperparameters\n",
      "â€“ Data problems\n",
      "â€“ Poor choice of features\n",
      "tried-and-true debugging techniques published by experienced ML engineers\n",
      "and researchers\n",
      "â€“ Start simple and gradually add more components\n",
      "â€“ Overfit a single batch\n",
      "â€“ Set a random seed\n",
      "Distributed Training\n",
      "Itâ€™s common to train a model using data that doesnâ€™t fit into memory.\n",
      "When a sample of your data is large, e.g., one machine can handle a few\n",
      "samples at a time, you might only be able to work with a small batch size, which\n",
      "leads to instability for gradient descent-based optimization.\n",
      "In some cases, a data sample is so large it canâ€™t even fit into memory and you\n",
      "will have to use something like gradient checkpointing, a technique that\n",
      "leverages the memory footprint and compute trade-off to make your system do\n",
      "more computation with less memory.\n",
      "Data parallelism\n",
      "you split your data on multiple machines, train your model on all of them, and\n",
      "accumulate gradients.\n",
      "Model parallelism\n",
      "Model parallelism is when different components of your model are trained on\n",
      "different machines\n",
      "Model parallelism can be misleading because in some cases parallelism doesnâ€™t\n",
      "mean that different parts of the model in different machines are executed in\n",
      "parallel.\n",
      "Pipeline parallelism is a clever technique to make different components of a\n",
      "model on different machines run more in parallel.\n",
      "When machine 1 finishes the first part of its computation, it passes the result\n",
      "onto machine 2, then continues to the second part, and so on.\n",
      "AutoML\n",
      "AutoML refers to automating the process of finding ML algorithms to solve real-\n",
      "world problems.\n",
      "One mild form, and the most popular form, of AutoML in production is\n",
      "hyperparameter tuning.\n",
      "hyperparameter = parameter supplied by users whose value is used to control\n",
      "the learning process, e.g., learning rate, batch size, number of hidden layers,\n",
      "number of hidden units, dropout probability, Î²1 and Î²2 in Adam optimizer, etc.\n",
      "With different sets of hyperparameters, the same model can give drastically\n",
      "different performances on the same dataset.\n",
      "Melis et al. showed in their 2018 paper â€œOn the State of the Art of Evaluation in\n",
      "Neural Language Modelsâ€ that weaker models with well-tuned hyperparameters\n",
      "can outperform stronger, fancier models.\n",
      "The goal of hyperparameter tuning is to find the optimal set of\n",
      "hyperparameters for a given model within a search space\n",
      "Popular ML frameworks either come with built-in utilities or have third-party\n",
      "utilities for hyperparameter tuningâ€”for example, scikit-learn with auto-\n",
      "sklearn,25 TensorFlow with Keras Tuner, and Ray with Tune.\n",
      "Popular methods for hyperparameter tuning include random search,26 grid\n",
      "search, and Bayesian optimization.\n",
      "When tuning hyperparameters, keep in mind that a modelâ€™s performance might\n",
      "be more sensitive to the change in one hyperparameter than another, and\n",
      "therefore sensitive hyperparameters should be more carefully tuned.\n",
      "Itâ€™s crucial to never use your test split to tune hyperparameters.\n",
      "Choose the best set of hyperparameters for a model based on its performance\n",
      "on a validation split, then report the modelâ€™s final performance on the test split.\n",
      "If you use your test split to tune hyperparameters, you risk overfitting your\n",
      "model to the test split.\n",
      "Hard AutoML\n",
      "Instead of manually putting a pooling layer after a convolutional layer or ReLu\n",
      "(rectified linear unit) after linear, you give your algorithm these building blocks\n",
      "and let it figure out how to combine them.\n",
      "= neural architecture search (NAS)\n",
      "Four Phases of ML Model Development\n",
      "1. Before machine learning\n",
      "If this is your first time trying to make this type of prediction from this\n",
      "type of data, start with non-ML solutions. Your first stab at the\n",
      "problem can be the simplest heuristics.\n",
      "2. Simplest machine learning models\n",
      "you want to start with a simple algorithm, something that gives you\n",
      "visibility into its working to allow you to validate the usefulness of your\n",
      "problem framing and your data.\n",
      "â€”> Logistic regression, gradient-boosted trees, k-nearest neighbor\n",
      "3. Optimizing simple models\n",
      "different objective functions, hyperparameter search, feature\n",
      "engineering, more data, and ensembles\n",
      "4. Complex models\n",
      "experiment with more complex models\n",
      "Model Offline Evaluation\n",
      "How do I know that our ML models are any good?\n",
      "the evaluation methods should be the same during both development and\n",
      "production. But in many cases, the ideal is impossible because you have\n",
      "ground truth labels during development, but in production, you donâ€™t.\n",
      "itâ€™s possible to infer or approximate labels in production based on usersâ€™\n",
      "feedback\n",
      "For other tasks, you might not be able to evaluate your modelâ€™s performance in\n",
      "production directly and might have to rely on extensive monitoring to detect\n",
      "changes and failures in your ML systemâ€™s performance.\n",
      "Baselines\n",
      "EXKURS: FID score\n",
      "â— a common metric for measuring the quality of synthesized images. The\n",
      "smaller the value, the higher the quality is supposed to be.\n",
      "Evaluation metrics, by themselves, mean little. When evaluating your model, itâ€™s\n",
      "essential to know the baseline youâ€™re evaluating it against.\n",
      "Five essential baselines, that might be useful:\n",
      "1. Random baseline\n",
      "If our model just predicts at random, whatâ€™s the expected\n",
      "performance?\n",
      "2. Simple heuristic\n",
      "Forget ML. If you just make predictions based on simple heuristics,\n",
      "what performance would you expect?\n",
      "3. Zero rule baseline\n",
      "The zero rule baseline is a special case of the simple heuristic baseline\n",
      "when your baseline model always predicts the most common class.\n",
      "4. Human baseline\n",
      "In many cases, the goal of ML is to automate what would have been\n",
      "otherwise done by humans, so itâ€™s useful to know how your model\n",
      "performs compared to human experts.\n",
      "5. Existing solutions\n",
      "ML systems are designed to replace existing solutions, which might be\n",
      "business logic with a lot of if/else statements or third-party solutions.\n",
      "Itâ€™s crucial to compare your new model to these existing solutions.\n",
      "When evaluating a model, itâ€™s important to differentiate between â€œa good\n",
      "systemâ€ and â€œa useful system.â€\n",
      "A good system isnâ€™t necessarily useful, and a bad system isnâ€™t necessarily\n",
      "useless.\n",
      "Evaluation Methods\n",
      "Perturbation tests\n",
      "The model that performs best on training data isnâ€™t necessarily the model that\n",
      "performs best on noisy data.\n",
      "To get a sense of how well your model might perform with noisy data, you can\n",
      "make small changes to your test splits to see how these changes affect your\n",
      "modelâ€™s performance.\n",
      "The more sensitive your model is to noise, the harder it will be to maintain it,\n",
      "since if your usersâ€™ behaviors change just slightly, such as they change their\n",
      "phones, your modelâ€™s performance might degrade.\n",
      "Invariance tests\n",
      "Certain changes to the inputs shouldnâ€™t lead to changes in the output.\n",
      "Discover biases: keep the inputs the same but change the sensitive information\n",
      "to see if the outputs change.\n",
      "Better, you should exclude the sensitive information from the features used to\n",
      "train the model in the first place.\n",
      "Directional expectation tests\n",
      "Certain changes to the inputs should, however, cause predictable changes in\n",
      "outputs.\n",
      "If the outputs change in the opposite expected direction, your model might not\n",
      "be learning the right thing, and you need to investigate it further before\n",
      "deploying it.\n",
      "Model calibration\n",
      "To quote Nate Silver in his book The Signal and the Noise, calibration is â€œone of\n",
      "the most important tests of a forecastâ€” I would argue that it is the single most\n",
      "important one.\n",
      "To measure a modelâ€™s calibration, a simple method is counting: you count the\n",
      "number of times your model outputs the probability X and the frequency Y of\n",
      "that prediction coming true, and plot X against Y. The graph for a perfectly\n",
      "calibrated model will have X equal Y at all data points.\n",
      "In scikit-learn, you can plot the calibration curve of a binary classifier with the\n",
      "method sklearn.calibration.calibration_curve\n",
      "Confidence measurement\n",
      "Confidence measurement can be considered a way to think about the\n",
      "usefulness threshold for each individual prediction.\n",
      "While most other metrics measure the systemâ€™s performance on average,\n",
      "confidence measurement is a metric for each individual sample.\n",
      "System-level measurement is useful to get a sense of overall performance, but\n",
      "sample-level metrics are crucial when you care about your systemâ€™s\n",
      "performance on every sample.\n",
      "Slice-based evaluation\n",
      "Slicing means to separate your data into subsets and look at your modelâ€™s\n",
      "perfor- mance on each subset separately.\n",
      "A common mistake that Iâ€™ve seen in many compa- nies is that they are focused\n",
      "too much on coarse-grained metrics like overall F1 or accuracy on the entire\n",
      "data and not enough on sliced-based metrics.\n",
      "The focus on overall performance is harmful not only because of the potential\n",
      "public backlash, but also because it blinds the company to huge potential\n",
      "model improvements.\n",
      "A fascinating and seemingly counterintuitive reason why slice-based evaluation\n",
      "is crucial is Simpsonâ€™s paradox, a phenomenon in which a trend appears in\n",
      "several groups of data but disappears or reverses when the groups are\n",
      "combined.\n",
      "This means that model B can perform better than model A on all data together,\n",
      "but model A performs better than model B on each subgroup separately.\n",
      "Even when you donâ€™t think slices matter, understanding how your model\n",
      "performs in a more fine-grained way can give you confidence in your model to\n",
      "convince other stakeholders, like your boss or your customers, to trust your ML\n",
      "models.\n",
      "To track your modelâ€™s performance on critical slices, youâ€™d first need to know\n",
      "what your critical slices are.\n",
      "1. Heuristics-based\n",
      "Slice your data using domain knowledge you have of the data and the\n",
      "task at hand.\n",
      "2. Error analysis\n",
      "Manually go through misclassified examples and find patterns among\n",
      "them.\n",
      "3. Slice finder\n",
      "The process generally starts with generating slice candidates with\n",
      "algorithms such as beam search, clustering, or decision, then pruning\n",
      "out clearly bad candidates for slices, and then ranking the candi-\n",
      "dates that are left.\n",
      "Chapter 7: Model deployment and prediction service\n",
      "To be deployed, your model will have to leave the devel- opment environment.\n",
      "Your model can be deployed to a staging environment for testing or to a\n",
      "production environment to be used by your end users.\n",
      "If you want to deploy a model for your friends to play with, all you have to do is\n",
      "to wrap your predict function in a POST request endpoint using Flask or\n",
      "FastAPI, put the dependencies this predict function needs to run in a\n",
      "container,2 and push your model and its associated container to a cloud service\n",
      "like AWS or GCP to expose the endpoint:\n",
      "The hard parts include making your model available to millions of users with a\n",
      "latency of milliseconds and 99% uptime, setting up the infrastructure so that\n",
      "the right person can be immediately notified when something goes wrong,\n",
      "figuring out what went wrong, and seamlessly deploying the updates to fix\n",
      "whatâ€™s wrong.\n",
      "Exporting a model means converting this model into a format that can be used\n",
      "by another application. Some people call this process â€œserialization.\n",
      "The process of generating predictions is called inference.\n",
      "Machine Learning Deployment Myths\n",
      "Myth 1: You Only Deploy One or Two ML Models at a Time\n",
      "In reality, companies have many, many ML models. An application might have\n",
      "many different features, and each feature might require its own model.\n",
      "Consider a ride-sharing app like Uber. It needs a model to predict each of the\n",
      "following elements: ride demand, driver availability, estimated time of arrival,\n",
      "dynamic pricing, fraudu- lent transaction, customer churn, and more.\n",
      "Additionally, if this app operates in 20 countries, until you can have models that\n",
      "generalize across different user-profiles, cultures, and languages, each country\n",
      "would need its own set of models. So with 20 countries and 10 models for each\n",
      "country, you already have 200 models.\n",
      "Myth 2: If We Donâ€™t Do Anything, Model Performance Remains the Same\n",
      "ML systems suffer from what are known as data distribution shifts, when the\n",
      "data distribution your model encounters in production is different from the data\n",
      "distribution it was trained on.\n",
      "Therefore, an ML model tends to perform best right after training and to\n",
      "degrade over time.\n",
      "Myth 3: You Wonâ€™t Need to Update Your Models as Much\n",
      "â€œHow often should I update my models?â€ Itâ€™s the wrong question to ask. The\n",
      "right question should be: â€œHow often can I update my models?â€\n",
      "While many companies still only update their models once a month, or even\n",
      "once a quarter, Weiboâ€™s iteration cycle for updating some of their ML models is\n",
      "10 minutes.\n",
      "Myth 4: Most ML Engineers Donâ€™t Need to Worry About Scale\n",
      "What â€œscaleâ€ means varies from application to application, but examples\n",
      "include a system that serves hundreds of queries per second or millions of\n",
      "users a month.\n",
      "Batch Prediction Versus Online Prediction\n",
      "One fundamental decision youâ€™ll have to make that will affect both your end\n",
      "users and developers working on your system is how it generates and serves its\n",
      "predictions to end users: online or batch.\n",
      "â— Batch prediction, which uses only batch features.\n",
      "â— Online prediction that uses only batch features (e.g., precomputed\n",
      "embeddings).\n",
      "â— Online prediction that uses both batch features and streaming\n",
      "features. This is also known as streaming prediction.\n",
      "Online prediction is when predictions are generated and returned as soon as\n",
      "requests for these predictions arrive.\n",
      "= on-demand prediction // synchronous prediction\n",
      "Batch prediction is when predictions are generated periodically or whenever\n",
      "triggered.\n",
      "The predictions are stored somewhere, such as in SQL tables or an in-memory\n",
      "database, and retrieved as needed.\n",
      "= asynchronous prediction\n",
      "Features computed from historical data, such as data in databases and data\n",
      "warehouses, are batch features.\n",
      "Features computed from streaming dataâ€”data in real-time transportsâ€”are\n",
      "streaming features.\n",
      "In batch prediction, only batch features are used. In online prediction, however,\n",
      "itâ€™s possible to use both batch features and streaming features.\n",
      "Online prediction and batch prediction donâ€™t have to be mutually exclusive. One\n",
      "hybrid solution is that you precompute predictions for popular queries, then\n",
      "generate predictions online for less popular queries.\n",
      "From Batch Prediction to Online Prediction\n",
      "The more natural way to serve predictions is probably online. You give your\n",
      "model an input and it generates a prediction as soon as it receives that input.\n",
      "You export your model, upload the exported model to Amazon SageMaker or\n",
      "Google App Engine, and get back an exposed endpoint. Now, if you send a\n",
      "request that contains an input to that endpoint, it will send back a prediction\n",
      "generated on that input.\n",
      "A problem with online prediction is that your model might take too long to\n",
      "generate predictions.\n",
      "Instead of generating predictions as soon as they arrive, what if you compute\n",
      "predictions in advance and store them in your database, and fetch them when\n",
      "requests arrive? This is exactly what batch prediction does.\n",
      "Batch prediction can also be seen as a trick to reduce the inference latency of\n",
      "more complex modelsâ€”the time it takes to retrieve a prediction is usually less\n",
      "than the time it takes to generate it.\n",
      "Batch prediction is good for when you want to generate a lot of predictions and\n",
      "donâ€™t need the results immediately.\n",
      "The problem with batch prediction is that it makes your model less responsive\n",
      "to usersâ€™ change preferences.\n",
      "Batch prediction is a workaround for when online prediction isnâ€™t cheap enough\n",
      "or isnâ€™t fast enough.\n",
      "Overcome the latency challenge of online prediction:\n",
      "â— A (near) real-time pipeline that can work with incoming data, extract\n",
      "streaming features (if needed), input them into a model, and return a\n",
      "prediction in near real-time. A streaming pipeline with real-time\n",
      "transport and a stream computation engine can help with that.\n",
      "â— A model that can generate predictions at a speed acceptable to its end\n",
      "users. For most consumer apps, this means milliseconds.\n",
      "Having two different pipelines to process your data is a common cause for bugs\n",
      "in ML production.\n",
      "One cause for bugs is when the changes in one pipeline arenâ€™t correctly\n",
      "replicated in the other, leading to two pipelines extracting two different sets of\n",
      "features. This is especially common if the two pipelines are maintained by two\n",
      "different teams.\n",
      "Building infrastructure to unify stream processing and batch processing has\n",
      "become a popular topic in recent years for the ML community.\n",
      "â€”> using a stream processor like Apache Flink\n",
      "Model Compression\n",
      "If the model you want to deploy takes too long to generate predictions:\n",
      "â€“ make it do inference faster (= inference optimization)\n",
      "â€“ make the model smaller (= model compression)\n",
      "â€“ make the hardware itâ€™s deployed on run faster\n",
      "â€”> Cheng et al.â€™s â€œSurvey of Model Compression and Acceleration for Deep\n",
      "Neural Networksâ€œ\n",
      "Low-Rank Factorization\n",
      "= replace high-dimensional tensors with lower-dimensional tensors\n",
      "One type of low-rank factorization is compact convolutional filters, where the\n",
      "over-parameterized (having too many parameters) convolution filters are\n",
      "replaced with compact blocks to both reduce the number of parameters and\n",
      "increase speed.\n",
      "Knowledge Distillation\n",
      "= a small model (student) is trained to mimic a larger model or ensemble of\n",
      "models (teacher)\n",
      "One example of a distilled network used in production is DistilBERT, which\n",
      "reduces the size of a BERT model by 40% while retaining 97% of its language\n",
      "understanding capabilities and being 60% faster\n",
      "advantage: it can work regardless of the architectural differences between the\n",
      "teacher and the student networks\n",
      "For example, you can get a random forest as the student and a transformer as\n",
      "the teacher\n",
      "disadvantage: itâ€™s highly dependent on the availability of a teacher network.\n",
      "Pruning\n",
      "1= remove entire nodes of a neural network, which means changing its\n",
      "architecture and reducing its number of parameters\n",
      "2= find parameters least useful to predictions and set them to 0\n",
      "The architecture of the neural network remains the same.\n",
      "This helps with reducing the size of a model because pruning makes a neural\n",
      "network more sparse, and sparse architecture tends to require less storage\n",
      "space than dense structure.\n",
      "Quantization\n",
      "= reduces a modelâ€™s size by using fewer bits to represent its parameters.\n",
      "Also improves the computation speed. First, it allows us to increase our batch\n",
      "size. Second, less precision speeds up computation, which further reduces\n",
      "training time and inference latency.\n",
      "ML on the Cloud and on the Edge\n",
      "On the cloud means a large chunk of computation is done on the cloud, either\n",
      "public clouds or private clouds.\n",
      "On the edge means a large chunk of computation is done on consumer devices\n",
      "â€”such as browsers, phones, laptops, smartwatches, cars, security cameras,\n",
      "robots, embedded devices, FPGAs (field programmable gate arrays), and ASICs\n",
      "(application-specific integrated circuits)â€”which are also known as edge\n",
      "devices.\n",
      "easiest way: deploy via a managed cloud service (AWS, GCP)\n",
      "downside: cost - ML models can be compute-intensive, and computing is\n",
      "expensive.\n",
      "https://blog.tomilkieway.com/72k-1/\n",
      "Putting your models on the edge is also appealing when handling sensitive user\n",
      "data.\n",
      "Edge computing makes it easier to comply with regulations, like GDPR, about\n",
      "how user data can be transferred or stored.\n",
      "Because of the many benefits that edge computing has over cloud computing,\n",
      "companies are in a race to develop edge devices optimized for different ML use\n",
      "cases.\n",
      "Compiling and Optimizing Models for Edge Devices\n",
      "For a model built with a certain framework, such as TensorFlow or PyTorch, to\n",
      "run on a hardware backend, that framework has to be supported by the\n",
      "hardware vendor.\n",
      "IRs (intermediate representations) lie at the core of how compilers work. From\n",
      "the original code for a model, compilers generate a series of high- and low-\n",
      "level IRs before generating the code native to a hardware backend so that it can\n",
      "run on that hardware backend.\n",
      "This process is also called lowering, as in you â€œlowerâ€ your high-level\n",
      "framework code into low-level hardware-native code. Itâ€™s not translating\n",
      "because thereâ€™s no one-to-one mapping between them.\n",
      "Model Optimization\n",
      "A typical ML workflow consists of many frameworks and libraries. For example,\n",
      "you might use pandas/dask/ray to extract features from your data. You might\n",
      "use NumPy to perform vectorization. You might use a pre-trained model like\n",
      "Hugging Faceâ€™s Transformers to generate features, then make predictions using\n",
      "an ensemble of models built with various frameworks like sklearn, TensorFlow,\n",
      "or LightGBM.\n",
      "In many companies, what usually happens is that data scientists and ML\n",
      "engineers develop models that seem to be working fine in development.\n",
      "However, when these models are deployed, they turn out to be too slow, so\n",
      "their companies hire optimization engineers to optimize their models for the\n",
      "hardware their models run on.\n",
      "There are two ways to optimize your ML models: locally and globally. Locally is\n",
      "when you optimize an operator or a set of operators of your model. Globally is\n",
      "when you optimize the entire computation graph end to end.\n",
      "Four common techniques for standard local optimization:\n",
      "1. Vectorization\n",
      "2. Parallelization\n",
      "3. Loop tiling\n",
      "4. Operator fusion\n",
      "Using ML to optimize ML models\n",
      "There are a couple of drawbacks to hand-designed heuristics. First, theyâ€™re\n",
      "nonoptimal. Thereâ€™s no guarantee that the heuristics an engineer comes up with\n",
      "are the best possible solution. Second, they are nonadaptive. Repeating the\n",
      "process on a new framework or a new hardware architecture requires an\n",
      "enormous amount of effort.\n",
      "If you donâ€™t have ideas for good heuristics, one possible solution might be to try\n",
      "all possible ways to execute a computation graph, record the time they need to\n",
      "run, then pick the best one.\n",
      "ML in Browsers\n",
      "WASM (WebAssembly) is an open standard that allows you to run executable\n",
      "programs in browsers. After youâ€™ve built your models in scikit-learn, PyTorch,\n",
      "TensorFlow, or whatever frameworks youâ€™ve used, instead of compiling your\n",
      "models to run on specific hardware, you can compile your model to WASM. You\n",
      "get back an executable file that you can just use with JavaScript.\n",
      "main drawback of WASM is that because WASM runs in browsers, itâ€™s slow.\n"
     ]
    }
   ],
   "source": [
    "import pdfplumber\n",
    "\n",
    "# Open the PDF file\n",
    "with pdfplumber.open(\"./__original-data/ðŸ“šBOOK NOTE : Huyen_2022_Designing Machine Learning Systems: an iterativeÂ â€¦.pdf\") as pdf:\n",
    "    # Loop through all pages\n",
    "    for page in pdf.pages:\n",
    "        # Extract the text from the current page\n",
    "        text = page.extract_text()\n",
    "        print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ignoring wrong pointing object 9 0 (offset 0)\n",
      "Ignoring wrong pointing object 23 0 (offset 0)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Document(page_content='1.2.3.4.5.\\nðŸ“šBOOK NOTE // Huyen_2022_Designing Machine Learning Systems: an iterative process for production-ready applications.Chapter 2: Introduction to Machine Learning Systems Design The ultimate goal of any project within a business is, therefore, to increase profits, either directly or indirectly: directly such as increasing sales (conversion rates) and cutting costs; indirectly such as higher customer satisfaction and increasing time spent on a website. What business performance metrics is the new ML system supposed to influence, e.g., the amount of ads revenue, the number of monthly active users? The longer youâ€™ve adopted ML, the more efficient your pipeline will run, the faster your development cycle will be, the less engineering time youâ€™ll need, and the lower your cloud bills will be, which all lead to higher returns. RequirementsReliability: With traditional software systems, you often get a warning, such as a system crash or runtime error or 404. However, ML systems can fail silently. Scalability: An ML system might grow in ML model count. Autoscaling is an indispensable feature in many cloud services: automatically scaling up and down the number of machines depending on usage. Maintainability: Code should be documented. Code, data, and artifacts should be versioned. Models should be sufficiently reproducible so that even when the original authors are not around, other contributors can have sufficient contexts to build on their work. Adaptability: ML systems need to be able to evolve quickly. Iterative Process: Once a system is put into production, itâ€™ll need to be continually monitored and updated. The process looks more like a cycle with a lot of back and forth between different steps. An ML problem is defined by inputs, outputs, and the objective function that guides the learning process. Classification models classify inputs into different categories. Regression models output a continuous value. A regression model can easily be framed as a classification model and vice versa.', metadata={'source': './__original-data/ðŸ“šBOOK NOTE : Huyen_2022_Designing Machine Learning Systems: an iterative\\xa0â€¦.pdf', 'page': 0})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "loader = PyPDFLoader(\"./__original-data/ðŸ“šBOOK NOTE : Huyen_2022_Designing Machine Learning Systems: an iterativeÂ â€¦.pdf\")\n",
    "pages = loader.load_and_split()\n",
    "\n",
    "pages[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "import tqdm as notebook_tqdm\n",
    "\n",
    "# EMBEDDINGS MODEL\n",
    "\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "hf_token = os.getenv(\"HF_EMBEDDINGS_MODEL\")\n",
    "embedding_url = \"https://api-inference.huggingface.co/pipeline/feature-extraction/sentence-transformers/all-MiniLM-L6-v2\"\n",
    "\n",
    "model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "model_kwargs = {'device': 'cpu'}\n",
    "encode_kwargs = {'normalize_embeddings': False}\n",
    "HFEmbeddings = HuggingFaceEmbeddings(\n",
    "    model_name=model_name,\n",
    "    model_kwargs=model_kwargs,\n",
    "    encode_kwargs=encode_kwargs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'HuggingFaceEmbeddings' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m text_splitter \u001b[38;5;241m=\u001b[39m SemanticChunker(\u001b[43mHFEmbeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[0;31mTypeError\u001b[0m: 'HuggingFaceEmbeddings' object is not callable"
     ]
    }
   ],
   "source": [
    "text_splitter = SemanticChunker(HFEmbeddings())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
