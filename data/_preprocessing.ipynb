{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdfplumber\n",
    "\n",
    "# Open the PDF file\n",
    "with pdfplumber.open(\"./__original-data/üìöBOOK NOTE : Huyen_2022_Designing Machine Learning Systems: an iterative¬†‚Ä¶.pdf\") as pdf:\n",
    "    # Loop through all pages\n",
    "    for page in pdf.pages:\n",
    "        # Extract the text from the current page\n",
    "        text = page.extract_text()\n",
    "        print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ignoring wrong pointing object 9 0 (offset 0)\n",
      "Ignoring wrong pointing object 23 0 (offset 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.2.3.4.5.\n",
      "üìöBOOK NOTE // Huyen_2022_Designing Machine Learning Systems: an iterative process for production-ready applications.Chapter 2: Introduction to Machine Learning Systems Design The ultimate goal of any project within a business is, therefore, to increase profits, either directly or indirectly: directly such as increasing sales (conversion rates) and cutting costs; indirectly such as higher customer satisfaction and increasing time spent on a website. What business performance metrics is the new ML system supposed to influence, e.g., the amount of ads revenue, the number of monthly active users? The longer you‚Äôve adopted ML, the more efficient your pipeline will run, the faster your development cycle will be, the less engineering time you‚Äôll need, and the lower your cloud bills will be, which all lead to higher returns. RequirementsReliability: With traditional software systems, you often get a warning, such as a system crash or runtime error or 404. However, ML systems can fail silently. Scalability: An ML system might grow in ML model count. Autoscaling is an indispensable feature in many cloud services: automatically scaling up and down the number of machines depending on usage. Maintainability: Code should be documented. Code, data, and artifacts should be versioned. Models should be sufficiently reproducible so that even when the original authors are not around, other contributors can have sufficient contexts to build on their work. Adaptability: ML systems need to be able to evolve quickly. Iterative Process: Once a system is put into production, it‚Äôll need to be continually monitored and updated. The process looks more like a cycle with a lot of back and forth between different steps. An ML problem is defined by inputs, outputs, and the objective function that guides the learning process. Classification models classify inputs into different categories. Regression models output a continuous value. A regression model can easily be framed as a classification model and vice versa.Within classification problems, the fewer classes there are to classify, the simpler the problem is. The simplest is binary classification, where there are only two possible classes. When there are more than two classes, the problem becomes multiclass classification.When the number of classes is high, we say the classification task has high cardinality.ML models typically need at least 100 examples for each class to learn to classify that class. So if you have 1,000 classes, you already need at least 100,000 examples. Solution: When the number of classes is large, hierarchical classification might be useful. In hierarchical classification, you have a classifier to first classify each example into one of the large groups. Then you have another classifier to classify this example into one of the subgroups. In both binary and multiclass classification, each example belongs to exactly one class. When an example can belong to multiple classes, we have a multilabel classification problem. To learn, an ML model needs an objective function to guide the learning process. An objective function is also called a loss function because the objective of the learning process is usually to minimize (or optimize) the loss caused by wrong predictions. Progress in the last decade shows that the success of an ML system depends largely on the data it was trained on. Instead of focusing on improving ML algorithms, most companies focus on managing and improving their data. Mind might be disguised as inductive biases or intelligent architectural designs. Data might be grouped together with computation since more data tends to require more computation. Judea Pearl (The Book of Why): ‚ÄúData is profoundly dumb.‚Äù ‚ÄúML will not be the same in 3‚Äì5 years, and ML folks who continue to follow the current data-centric paradigm will find themselves outdated, if not jobless. Take note.‚Äù Peter Norvig (Google director of search quality): ‚ÄúWe don‚Äôt have better algorithms. We just have more data.‚Äù Every project must start with why this project needs to happen, and ML projects are no exception.‚óè‚óè‚óè1.Chapter 3: Data Engineering FundamentalsIf data models describe the data in the real world, databases specify how the data should be stored on machines. Data SourcesUnderstanding the sources your data comes from can help you use your data more efficiently. One source is user input data, data explicitly input by users. User input data can be easily malformatted. System-generated data is the data generated by different components of your systems, which include various types of logs and system outputs such as model predictions. Because debugging ML systems is hard, it‚Äôs a common practice to log everything you can. To analyze logs: Logstash, Datadog, Logz.io, etc. Many of them use ML models to help you process and make sense of your massive number of logs. There are also internal databases, generated by various services and enterprise applications in a company. These databases manage their assets such as inventory, customer relationships, users, and more. First-party data is the data that your company already collects about your users or customers. Second-party data is the data collected by another company on their own customers that they make available to you, though you‚Äôll probably have to pay for it. Third-party data companies collect data on the public who aren‚Äôt their direct customers. Data FormatsHow do I store multimodal data, e.g., a sample that might contain both images and texts? Where do I store my data so that it‚Äôs cheap and still fast to access?How do I store complex models so that they can be loaded and run correctly on different hardware? The process of converting a data structure or object state into a format that can be stored or transmitted and reconstructed later is data serialization. JSON = JavaScript Object Notation: Its key-value pair paradigm is simple but powerful, capable of handling data of different levels of1.2.\n",
      "3.\n",
      "4.structuredness. Row-Major Versus Column-Major Format: CSV (comma-separated values) is row-major, which means consecutive elements in a row are stored next to each other in memory. Parquet is column-major, which means consecutive elements in a column are stored next to each other.‚Äî> Because modern computers process sequential data more efficiently than nonsequential data, if a table is row-major, accessing its rows will be faster than accessing its columns in expectation. This means that for row-major formats, accessing data by rows is expected to be faster than accessing data by columns. NumPy Versus pandas- pandas is built around DataFrame, a concept inspired by R‚Äôs Data Frame, which is column-major. A DataFrame is a two-dimensional table with rows and columns. - In NumPy, the major order can be specified. When an ndarray is created, it‚Äôs row-major by default if you don‚Äôt specify the order. ‚Äî> Accessing a DataFrame by row is so much slower than accessing the same DataFrame by column. Text Versus Binary Format: Text files are files that are in plain text, which usually means they are human-readable. Binary files are the catchall that refers to all nontext files. As the name suggests, binary files are typically files that contain only 0s and 1s, and are meant to be read or used by programs that know how to interpret the raw bytes.‚Äî> You want to store the number 1000000. If you store it in a text file, it‚Äôll require 7 characters, and if each character is 1 byte, it‚Äôll require 7 bytes. If you store it in a binary file as int32, it‚Äôll take only 32 bits or 4 bytes. => AWS recommends using the Parquet format because ‚Äúthe Parquet format is up to 2x faster to unload and consumes up to 6x less storage in Amazon S3, compared to text formats.‚Äù Data ModelsData models describe how data is represented. How you choose to represent data not only affects the way your systems are built but also the problems your systems can solve. Relational ModelsData is organized into relations; each relation is a set of tuples. A table is an accepted visual representation of a relation, and each row of a table makes up a tuple.One major downside of normalization is that your data is now spread across multiple relations. You can join the data from different relations back together, but joining can be expensive for large tables. Databases built around the relational data model are relational databases. The language that you can use to specify the data that you want from a database is called a query language. The most popular query language for relational databases today is SQL. SQL = declarative languageimperative paradigm = you specify the steps needed for an action and the computer executes these steps to return the outputs. declarative paradigm = you specify the outputs you want, and the computer figures out the steps needed to get you the queried outputs. SQL can be Turing-complete, which means that, in theory, SQL can be used to solve any computation problem (without making any guarantee about the time or memory required). (https://wiki.postgresql.org/wiki/Cyclic_Tag_System)Declarative ML Systems:https://docs.h2o.ai/h2o/latest-stable/h2o-docs/automl.htmlhttps://github.com/ludwig-ai/ludwigNoSQLTwo major types of nonrelational models are the document model and the graph model. The document model targets use cases where data comes in self-contained documents and relationships between one document and another are rare. The graph model goes in the opposite direction, targeting use cases where relationships between data items are common and important. Compared to the relational model, it‚Äôs harder and less efficient to execute joins across documents compared to across tables.Structured Versus Unstructured Data Structured data follows a predefined data model, also known as a data schema. The disadvantage of structured data is that you have to commit your data to a predefined schema. If your schema changes, you‚Äôll have to retrospectively update all your data, often causing mysterious bugs in the process. Unstructured data doesn‚Äôt adhere to a predefined data schema. It‚Äôs usually text but can also be numbers, dates, images, audio, etc. Unstructured data also allows for more flexible storage options. A repository for storing structured data is called a data warehouse.Data warehouses are used to store data that has been processed into formats ready to be used. A repository for storing unstructured data is called a data lake. Data lakes are usually used to store raw data before processing. Data Storage Engines and Processing Data formats and data models specify the interface for how users can store and retrieve data. Storage engines, also known as databases, are the implementation of how data is stored and retrieved on machines. Transactional and Analytical Processing The transactions are inserted as they are generated, and occasionally updated when something changes, or deleted when they are no longer needed (= online transaction processing (OLTP).Transactional databases are designed to process online transactions and satisfy the low latency, high availability requirements. Analytical questions require aggregating data in columns across multiple rows of data. Analytical databases are designed for this purpose (= online analytical processing (OLAP).ETL: Extract, Transform, and Load When data is extracted from different sources, it‚Äôs first transformed into the desired format before being loaded into the target destination such as a database or a data warehouse. In the extracting phase, you need to validate your data and reject the data that doesn‚Äôt meet your requirements. Transform is, where most of the data processing is done. You might want to join data from multiple sources and clean it. You might want to standardize the value ranges.Load is deciding how and how often to load your transformed data into the target destination, which can be a file, a database, or a data warehouse. Finding it difficult to keep data structured, some companies had this idea: ‚ÄúWhy not just store all data in a data lake so we don‚Äôt have to deal with schema changes? This process of loading data into storage first and then processing it later is sometimes called ELT (extract, load, transform).‚óè‚óè‚óã‚óã‚óã‚óã‚óè‚óã‚óã‚óãBUT: It‚Äôs inefficient to search through a massive amount of raw data for the data that you want. BUT 2: As companies switch to running applications on the cloud and infrastructures become standardized, data structures also become standardized. Modes of Dataflow How do we pass data between different processes that don‚Äôt share memory? When data is passed from one process to another, we say that the data flows from one process to another, which gives us a dataflow. Data passing through databases Data passing through services using requests such as the requests provided by REST and RPC APIs (e.g., POST/GET requests)Because processes communicate through requests, we say that this is request-driven.The most popular styles of requests used for passing data through networks are REST (representational state transfer) and RPC (remote procedure call) Implementations of a REST architecture are said to be RESTful.REST doesn‚Äôt exactly mean HTTP because HTTP is just an implementation of REST. Data passing through a real-time transport like Apache Kafka and Amazon Kinesis each service can write data to a database and other services that need the data can read from that database. A piece of data broadcast to a real-time transport is called an event. This architecture is, therefore, also called event-driven. pubsub (publish-subscribe) model = any service can publish to different topics in a real-time transport, and any service that subscribes to a topic can read all the events in that topic. The services that produce data don‚Äôt care about what services consume their data. Batch Processing Versus Stream Processing Once your data arrives in data storage engines like databases, data lakes, or data warehouses, it becomes historical data. Historical data is often processed in batch jobs‚Äîjobs that are kicked off periodically.‚ÄìWhen data is processed in batch jobs, we refer to it as batch processing. When you have data in real-time transports like Apache Kafka and Amazon Kinesis, we say that you have streaming data. Stream processing refers to doing computation on streaming data. Stream processing, when done right, can give low latency because you can process data as soon as data is generated, without having to first write it into databases. Batch features‚Äîfeatures extracted through batch processing‚Äîare also known as static features. Streaming features‚Äî features extracted through stream processing‚Äîare also known as dynamic features. Chapter 4: Training DataData is full of potential biases. These biases have many possible causes. There are biases caused during collecting, sampling, or labeling. Historical data might be embedded with human biases, and ML models, trained on this data, can perpetuate them. Use data but don‚Äôt trust it too much! SamplingSampling happens in many steps of an ML project lifecycle, such as sampling from all possible real-world data to create training data; sampling from a given dataset to create splits for training, validation, and testing, etc.Nonprobability Sampling Nonprobability sampling is when the selection of data isn‚Äôt based on any probability criteria. Beispiel: Convenience sampling Samples of data are selected based on their availability. This sampling method is popular because, well, it‚Äôs convenient.  The samples selected by nonprobability criteria are not representative of the real-world data and therefore are riddled with selection biases. Language models are often trained not with data that is representative of all possible texts but with data that can be easily collected‚ÄîWikipedia, Common Crawl, Reddit. Simple Random SamplingIn the simplest form of random sampling, you give all samples in the population equal probabilities of being selected. For example, you randomly select 10% of the population, giving all members of this population an equal 10% chance of being selected. Stratified Sampling To avoid the drawback of simple random sampling, you can first divide your population into the groups that you care about and sample from each group separately. Weighted Sampling Each sample is given a weight, which determines the probability of it being selected. This method allows you to leverage domain expertise. Reservoir Sampling One solution for this problem is reservoir sampling. The algorithm involves a reservoir, which can be an array.Importance Sampling It allows us to sample from a distribution when we only have access to another distribution. Labeling The performance of an ML model still depends heavily on the quality and quantity of the labeled data it‚Äôs trained on. Hand Labels Hand-labeling data can be expensive, especially if subject matter expertise is required. Hand labeling poses a threat to data privacy Hand labeling is slow Slow labeling leads to slow iteration speed and makes your model less adaptive to changing environments and requirements. If the task changes or data changes, you‚Äôll have to wait for your data to be relabeled before updating your model.Label multiplicity To obtain enough labeled data, companies have to use data from multiple sources and rely on multiple annotators who have different levels of expertise. These different data sources and annotators also have different levels of accuracy. Data lineage It‚Äôs good practice to keep track of the origin of each of your data samples as well as its labels, a technique known as data lineage. Data lineage helps you both flag potential biases in your data and debug your models. Natural Labels Tasks with natural labels are tasks where the model‚Äôs predictions can be automatically evaluated or partially evaluated by the system. Semi-supervision A classic semi-supervision method is self-training. You start by training a model on your existing set of labeled data and use this model to make predictions for unlabeled samples. Assuming that predictions with high raw probability scores are correct, you add the labels predicted with high probability to your training set and train a new model on this expanded training set. This goes on until you‚Äôre happy with your model performance. Semi-supervision is the most useful when the number of training labels is limited. Transfer learning Transfer learning refers to the family of methods where a model developed for a task is reused as the starting point for a model on a second task. Transfer learning is especially appealing for tasks that don‚Äôt have a lot of labeled data. Active learning Active learning is a method for improving the efficiency of data labels. Instead of randomly labeling data samples, you label the samples that are most help- ful to your models according to some metrics or heuristics.Class Imbalance Class imbalance typically refers to a problem in classification tasks where there is a substantial difference in the number of samples in each class of the training data. ML, especially deep learning, works well in situations when the data distribution is more balanced, and usually not so well when the classes are heavily imbalanced Precision, Recall, and F1 true positive rate (also known as recall) false positive rate (also known as the probability of false alarm) We can plot the true positive rate against the false positive rate for different thresholds. This plot is known as the ROC curve (receiver operating characteristics) ‚Äî> When your model is perfect, the recall is 1.0, and the curve is just a line at the top The area under the curve (AUC) measures the area under the ROC curve. Since the closer to the perfect line the better, the larger this area the better Data-level methods: Resampling Data-level methods modify the distribution of the training data to reduce the level of imbalance to make it easier for the model to learn. A common family of techniques is resampling. Resampling includes oversampling, adding more instances from the minority classes, and undersampling, removing instances of the majority classes. When you resample your training data, never evaluate your model on resampled data, since it will cause your model to overfit to that resampled distribution. Undersampling runs the risk of losing important data from removing data. Data Augmentation Data augmentation is a family of techniques that are used to increase the amount of training data.In computer vision, the simplest data augmentation technique is to randomly modify an image while preserving its label. In NLP, you can randomly replace a word with a similar word, assuming that this replacement wouldn‚Äôt change the meaning or the sentiment of the sentence.Perturbation (St√∂rung)Perturbation is also a label-preserving operation Using deceptive data to trick a neural network into making wrong predictions is called adversarial attacks. Adding noise to samples is a common technique to create adversarial samples. Data Synthesis Since collecting data is expensive and slow, with many potential privacy concerns, it‚Äôd be a dream if we could sidestep it altogether and train our models with synthesized data. Even though we‚Äôre still far from being able to synthesize all training data, it‚Äôs possible to synthesize some training data to boost a model‚Äôs performance. Chapter 5: Feature EngineeringState-of-the-art model architectures can still perform poorly if they don‚Äôt use a good set of features. A large part of many ML engineering and data science jobs is to come up with new useful features. Learned Features Versus Engineered Features The promise of deep learning is that we won‚Äôt have to handcraft features. For this reason, deep learning is sometimes called feature learning. Many features can be automatically learned and extracted by algorithms. However, we‚Äôre still far from the point where all features can be automated. EXKURS n-gram: an n-gram is a contiguous sequence of n items from a given sample of text. The items can be phonemes, syllables, letters, or words. For example, given the post ‚ÄúI like food,‚Äù its word-level 1-grams are [‚ÄúI‚Äù , ‚Äúlike‚Äù , ‚Äúfood‚Äù] and its word-level 2-grams are [‚ÄúI like‚Äù , ‚Äúlike food‚Äù].Once you‚Äôve generated n-grams for your training data, you can create a vocabulary that maps each n-gram to an index. Then you can convert each post into a vector based on its n-grams‚Äô indices. Feature engineering requires knowledge of domain-specific techniques‚Äîin this case, the domain is natural language processing (NLP) and the native language of the text. With Deep Learning: Instead of having to worry about lemmatization, punctuation, or stopword removal, you can just split your raw text into words (i.e., tokenization), create a vocabulary out of those words, and convert each of your words into one-shot vectors using this vocabulary. The process of choosing what information to use and how to extract this information into a format usable by your ML models is feature engineering. For complex tasks such as recommending videos for users to watch next on TikTok, the number of features used can go up to millions.For domain-specific tasks, you might need subject matter expertise. Common Feature Engineering Operations Handling Missing Values Missing not at random (MNAR) This is when the reason a value is missing is because of the true value itself. Missing at random (MAR)This is when the reason a value is missing is not due to the value itself, but due to another observed variable. Missing completely at random (MCAR)This is when there‚Äôs no pattern in when the value is missing. This type of missing is very rare. There are usually reasons why certain values are missing, and you should investigate. Solution:One way to delete is column deletion: if a variable has too many missing values, just remove that variable. Another way to delete is row deletion: if a sample has missing value(s), just remove that sample. However, removing rows of data can also remove important information thatyour model needs to make predictions (especially when MNAR)On top of that, removing rows of data can create biases in your model (especially when MAR)If you don‚Äôt want to delete missing values, you will have to impute them, which means ‚Äúfill them with certain values.‚Äù One common practice is to fill in missing values with their defaults (or with median, mean or mode - most common value).You want to avoid filling missing values with possible values.Multiple techniques might be used at the same time or in sequence to handle missing values for a particular set of data. ‚Äî> There is no perfect way to handle missing values Scaling Before inputting features into models, it‚Äôs important to scale them to similar ranges. This process is called feature scaling. This is one of the simplest things you can do that often results in a performance boost for your model. An intuitive way to scale your features is to get them to be in the range [0, 1]. If you think that your variables might follow a normal distribution, it might be helpful to normalize them so that they have zero mean and unit variance (standardization).In many cases, the log transformation can help reduce the skewness of your data.Encoding Categorical Features In production, categories change. One solution to this problem (dynamic change in categories) is the hashing trick.The gist of this trick is that you use a hash function to generate a hashed value of each category. The hashed value will become the index of that category. Because you can specify the hash space, you can fix the number of encoded values for a feature in advance, without having to know how many categoriesthere will be. One problem with hashed functions is collision: two categories being assigned the same index. You can choose a hash space large enough to reduce the collision. Feature Crossing Feature crossing is the technique to combine two or more features to generate new features. This technique is useful to model the nonlinear relationships between features. It‚Äôs less important in neural networks, but it can still be useful because explicit feature crossing occasionally helps neural networks learn nonlinear relationships faster. Discrete and Continuous Positional Embeddings Consider the task of language modeling where you want to predict the next token (e.g., a word, character, or subword) based on the previous sequence of tokens. EXKURS embeddings: An embedding is a vector that represents a piece of data. We call the set of all possible embeddings generated by the same algorithm for a type of data ‚Äúan embedding space.‚Äù All embedding vectors in the same space are of the same size. The embedding size for positions is usually the same as the embedding size for words so that they can be summed. Because the embeddings change as the model weights get updated, we say that the position embeddings are learned. The embedding for each position is still a vector with S elements (S is the position embedding size), but each element is predefined using a function, usually sine and cosine. In the original Transformer paper, if the element is at an even index, use sine. Else, use cosine. Fixed positional embedding is a special case of what is known as Fourier features. If positions in positional embeddings are discrete, Fourier features can also be continuous. Data LeakageData leakage refers to the phenomenon when a form of the label ‚Äúleaks‚Äù into the set of features used for making predictions, and this same information is not available during inference. It‚Äôs dangerous because it can cause your models to fail in an unexpected and spectacular way Splitting time-correlated data randomly instead of by time In many cases, data is time-correlated, which means that the time the data is generated affects its label distribution. To prevent future information from leaking into the training process and allowing models to cheat during evaluation, split your data by time, instead of splitting randomly, whenever possible. For example, if you have data from five weeks, use the first four weeks for the train split, then randomly split week 5 into validation and test.Scaling before splitting One common mistake is to use the entire training data to generate global statistics before splitting it into different splits, leaking the mean and variance of the test samples into the training process, and allowing a model to adjust its predictions for the test samples. To avoid this type of leakage, always split your data first before scaling, then use the statistics from the train split to scale all the splits. Filling in missing data with statistics from the test split Leakage might occur if the mean or median is calculated using the entire data instead of just the train split. It can be prevented by using only statistics from the train split to fill in missing values in all the splits. Poor handling of data duplication before splitting If you have duplicates or near-duplicates in your data, failing to remove them before splitting your data might cause the same samples to appear in both train and validation/test splits. Data duplication can result from data collection or merging of different data sources.To avoid this, always check for duplicates before splitting and also after splitting just to make sure. If you oversample your data, do it after splitting. Group leakage A group of examples have strongly correlated labels but are divided into different splits. his type of leakage is common for objective detection tasks that contain photos of the same object taken milliseconds apart‚Äîsome of them landed in the train split while others landed in the test split. It‚Äôs hard avoiding this type of data leakage without understanding how your data was generated. Leakage from data generation process Detecting this type of data leakage requires a deep understanding of the way data is collected. There‚Äôs no foolproof way to avoid this type of leakage, but you can mitigate the risk by keeping track of the sources of your data and understanding how it is collected and processed. Normalize your data so that data from different sources can have the same means and variances. Detecting Data Leakage Measure the predictive power of each feature or a set of features with respect to the target variable (label). If a feature has an unusually high correlation, investigate how this feature is generated and whether the correlation makes sense. Do ablation studies to measure how important a feature or a set of features is to your model. If removing a feature causes the model‚Äôs performance to deteriorate significantly, investigate why that feature is so important. Keep an eye out for new features added to your model. If adding a new feature significantly improves your model‚Äôs performance, either that feature is really good or that feature just contains leaked information about labels. Be very careful every time you look at the test split.If you use the test split in any way other than to report a model‚Äôs final performance, you risk leaking information from the future into your training process. Engineering Good Features Generally, adding more features leads to better model performance. If a feature doesn‚Äôt help a model make good predictions, regularization techniques like L1 regularization should reduce that feature‚Äôs weight to 0. Feature Importance If you use a classical ML algorithm like boosted gradient trees, the easiest way to measure the importance of your features is to use built-in feature importance functions implemented by XGBoost. For more model-agnostic methods, you might want to look into SHAP (SHapley Additive exPlanations).SHAP is great because it not only measures a feature‚Äôs importance to an entire model, but it also measures each feature‚Äôs contribution to a model‚Äôs specific prediction. InterpretML: https://github.com/interpretml/interpretOften, a small number of features accounts for a large portion of your model‚Äôs feature importance. Feature Generalization Not all features generalize equally. Measuring feature generalization is a lot less scientific than measuring feature importance, and it requires both intuition and subject matter expertise on top of statistical knowledge. Coverage is the percentage of the samples that has values for this feature in the data‚Äî so the fewer values that are missing, the higher the coverage. feature distribution: If the set of values that appears in the seen data (such as the train split) has no overlap with the set of values that appears in the unseen data (such as the test split), this feature might even hurt your model‚Äôs performanceChapter 6: Model development and Offline EvaluationIt allows to play around with different algorithms and techniques, even the latest ones.To build an ML model, we first need to select the ML model to build. Model development is an iterative process. After each iteration, you‚Äôll want to com- pare your model‚Äôs performance against its performance in previous iterations and evaluate how suitable this iteration is for production. Model EvaluationIf you had unlimited time and compute power, the rational thing to do would be to try all possible solutions and see what is best for you. However, time and compute power are limited resources, and you have to be strategic about what models you select. However, even though deep learning is finding more use cases in production, classical ML algorithms are not going away. Many recommender systems still rely on collaborative filtering and matrix factorization. Tree-based algorithms, including gradient-boosted trees, still power many classifica- tion tasks with strict latency requirements. Example: A k-means clustering model might be used to extract features to input into a neural network. When selecting a model for your problem, you don‚Äôt choose from every possible model out there, but usually focus on a set of models suitable for your problem. Example: If your boss tells you to build a system to detect toxic tweets, you know that this is a text classification problem‚Äîgiven a piece of text, classify whether it‚Äôs toxic or not‚Äîand common models for text classification include naive Bayes, logistic regression, recurrent neural networks, and transformer-based models such as BERT, GPT, and their variants. Knowledge of common ML tasks and the typical approaches to solve them is essential in this process. Different types of algorithms require different numbers of labels as well as different amounts of compute power. When considering what model to use, it‚Äôs important to consider not only the model‚Äôs performance, measured by metrics such as accuracy, F1 score, and log loss, but also its other properties, such as how much data, compute, and time it needs to train, what‚Äôs its inference latency, and interpretability.1.2.\n",
      "3.\n",
      "4.\n",
      "5.To understand different algorithms, the best way is to equip yourself with basic ML knowledge and run experiments with the algorithms you‚Äôre interested in. Important: Follow major ML conferences such as NeurIPS, ICLR, and ICML, as well as following researchers whose work has a high signal-to-noise ratio on Twitter. Six tips for model selection Avoid the state-of-the-art trap‚Äî> What means ‚Äûstate-of-the-art‚Äú?Being state-of-the-art often means that it performs better than existing models on some static datasets. It doesn‚Äôt mean that this model will be fast enough or cheap enough for you to implement.If there‚Äôs a solution that can solve your problem that is much cheaper and simpler than state-of-the-art models, use the simpler solution. Start with the simplest models Zen of Python states that ‚Äúsimple is better than complex,‚Äù 1. simpler models are easier to deploy 2. easier to understand your model 3. simplest model serves as a baseline ‚Äî> pretrained BERT models are complex, but they require little effort to get started with, especially if you use a ready-made implementation like the one in Hugging Face‚Äôs Transformer. Avoid human biases in selecting models Part of the process of evaluating an ML architecture is to experiment with different features and different sets of hyperparameters to find the best model of that architecture. If an engineer is more excited about an architecture, they will likely spend a lot more time experimenting with it, which might result in better-performing models for that architecture. When comparing different architectures, it‚Äôs important to compare them under comparable setups. Evaluate good performance now versus good performance laterA simple way to estimate how your model‚Äôs performance might change with more data is to use learning curves Learning Curve: a plot of its performance‚Äîe.g., training loss, training accuracy, validation accuracy‚Äîagainst the number of training samples it uses While evaluating models, you might want to take into account their potential for improvements in the near future, and how easy/difficult it is to achieve those improvements. Evaluate trade-offs A more complex model can give a better performance, but its results5.6.\n",
      "1.‚Äì‚Äì‚Äì‚Äì1.‚Äì‚Äì‚Äì1.‚Äì‚Äìare less interpretable. Understand your model‚Äôs assumptions The real world is intractably complex, and models can only approximate using assumptions. Ensembles One method that has consistently given a performance boost is to use an ensemble of multiple models instead of just an individual model to make predictions. Each model in the ensemble is called a base learner. Ensembling methods are less favored in production because ensembles are more complex to deploy and harder to maintain. When creating an ensemble, the less correlation there is among base learners, the better the ensemble will be. Therefore, it‚Äôs common to choose very different types of models for an ensemble. Bagging = bootstrap aggregating designed to improve both the training stability and accuracy of ML algorithms. reduces variance and helps to avoid overfitting. Given a dataset, instead of training one classifier on the entire dataset, you sample with replacement to create different datasets, called bootstraps, and train a classification or regression model on each of these bootstraps. Why Sampling? Sampling with replacement ensures that each bootstrap is created independently from its peers. Boosting = a family of iterative ensemble algorithms that convert weak learners to strong ones Each learner in this ensemble is trained on the same set of samples, but the samples are weighted differently among iterations. future weak learners focus more on the examples that previous weak learners misclassified. XGBoost, a variant of GBM, used to be the algorithm of choice for many winning teams in ML competitions. Stacking you train base learners from the training data and then create a meta-learner that combines the outputs of the base learners to output final predictions meta-learner can be as simple as a heuristic: you take the majority‚Äìvote (for classification tasks) or the average vote (for regression tasks) from all base learners. Experiment Tracking and Versioning It‚Äôs important to keep track of all the definitions needed to re-create an experiment and its relevant artifacts. An artifact is a file generated during an experiment‚Äîexamples of artifacts can be files that show the loss curve, evaluation loss graph, etc.The process of tracking the progress and results of an experiment is called experiment tracking. The process of logging all the details of an experiment for the purpose of possibly recreating it later or comparing it with other experiments is called versioning. Experiment tracking = babysitting the learning processes It‚Äôs important to track what‚Äôs going on during training not only to detect and address these issues but also to evaluate whether your model is learning anything useful. What should/could be tracked:Loss curve, model performance metrics, corresponding sample, prediction, ground truth label, speed of model, system performance metrics, parameter, hyperparameterA simple way to track your experiments is to automatically make copies of all the code files needed for an experiment and log all outputs with their timestamps. Versioning ML systems are part code, part data, so you need to not only version your code but your data as well. Code versioning tools allow users to revert to a previous version of the codebase by keeping copies of all the old files. Debugging ML Models ML models fail silently‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äìeven when you think you‚Äôve found the bug, it can be frustratingly slow to validate whether the bug has been fixed debugging ML models is hard because of their cross-functional complexity. There are many components in an ML system: data, labels, features, ML algorithms, code, infrastructure, etc. These different components might be owned by different teams. some of the things that might cause an ML model to fail: Theoretical constraints Poor implementation of model Poor choice of hyperparameters Data problems Poor choice of features tried-and-true debugging techniques published by experienced ML engineers and researchers Start simple and gradually add more components Overfit a single batch Set a random seed Distributed Training It‚Äôs common to train a model using data that doesn‚Äôt fit into memory. When a sample of your data is large, e.g., one machine can handle a few samples at a time, you might only be able to work with a small batch size, which leads to instability for gradient descent-based optimization. In some cases, a data sample is so large it can‚Äôt even fit into memory and you will have to use something like gradient checkpointing, a technique that leverages the memory footprint and compute trade-off to make your system do more computation with less memory. Data parallelism you split your data on multiple machines, train your model on all of them, and accumulate gradients. Model parallelism Model parallelism is when different components of your model are trained on different machinesModel parallelism can be misleading because in some cases parallelism doesn‚Äôt mean that different parts of the model in different machines are executed in parallel. Pipeline parallelism is a clever technique to make different components of a model on different machines run more in parallel. When machine 1 finishes the first part of its computation, it passes the result onto machine 2, then continues to the second part, and so on. AutoML AutoML refers to automating the process of finding ML algorithms to solve real-world problems. One mild form, and the most popular form, of AutoML in production is hyperparameter tuning. hyperparameter = parameter supplied by users whose value is used to control the learning process, e.g., learning rate, batch size, number of hidden layers, number of hidden units, dropout probability, Œ≤1 and Œ≤2 in Adam optimizer, etc. With different sets of hyperparameters, the same model can give drastically different performances on the same dataset. Melis et al. showed in their 2018 paper ‚ÄúOn the State of the Art of Evaluation in Neural Language Models‚Äù that weaker models with well-tuned hyperparameters can outperform stronger, fancier models. The goal of hyperparameter tuning is to find the optimal set of hyperparameters for a given model within a search space Popular ML frameworks either come with built-in utilities or have third-party utilities for hyperparameter tuning‚Äîfor example, scikit-learn with auto-sklearn,25 TensorFlow with Keras Tuner, and Ray with Tune. Popular methods for hyperparameter tuning include random search,26 grid search, and Bayesian optimization. When tuning hyperparameters, keep in mind that a model‚Äôs performance might be more sensitive to the change in one hyperparameter than another, and therefore sensitive hyperparameters should be more carefully tuned. It‚Äôs crucial to never use your test split to tune hyperparameters. Choose the best set of hyperparameters for a model based on its performance on a validation split, then report the model‚Äôs final performance on the test split. If you use your test split to tune hyperparameters, you risk overfitting your1.2.3.4.model to the test split. Hard AutoMLInstead of manually putting a pooling layer after a convolutional layer or ReLu (rectified linear unit) after linear, you give your algorithm these building blocks and let it figure out how to combine them. = neural architecture search (NAS) Four Phases of ML Model Development Before machine learning If this is your first time trying to make this type of prediction from this type of data, start with non-ML solutions. Your first stab at the problem can be the simplest heuristics. Simplest machine learning models you want to start with a simple algorithm, something that gives you visibility into its working to allow you to validate the usefulness of your problem framing and your data. ‚Äî> Logistic regression, gradient-boosted trees, k-nearest neighborOptimizing simple models different objective functions, hyperparameter search, feature engineering, more data, and ensembles Complex models experiment with more complex models Model Offline Evaluation How do I know that our ML models are any good? the evaluation methods should be the same during both development and production. But in many cases, the ideal is impossible because you have ground truth labels during development, but in production, you don‚Äôt. it‚Äôs possible to infer or approximate labels in production based on users‚Äô feedback For other tasks, you might not be able to evaluate your model‚Äôs performance in production directly and might have to rely on extensive monitoring to detect changes and failures in your ML system‚Äôs performance. Baselines‚óè1.2.3.4.5.EXKURS: FID score a common metric for measuring the quality of synthesized images. The smaller the value, the higher the quality is supposed to be. Evaluation metrics, by themselves, mean little. When evaluating your model, it‚Äôs essential to know the baseline you‚Äôre evaluating it against. Five essential baselines, that might be useful:Random baselineIf our model just predicts at random, what‚Äôs the expected performance? Simple heuristic Forget ML. If you just make predictions based on simple heuristics, what performance would you expect? Zero rule baseline The zero rule baseline is a special case of the simple heuristic baseline when your baseline model always predicts the most common class. Human baseline In many cases, the goal of ML is to automate what would have been otherwise done by humans, so it‚Äôs useful to know how your model performs compared to human experts. Existing solutions ML systems are designed to replace existing solutions, which might be business logic with a lot of if/else statements or third-party solutions. It‚Äôs crucial to compare your new model to these existing solutions. When evaluating a model, it‚Äôs important to differentiate between ‚Äúa good system‚Äù and ‚Äúa useful system.‚Äù A good system isn‚Äôt necessarily useful, and a bad system isn‚Äôt necessarily useless. Evaluation Methods Perturbation tests The model that performs best on training data isn‚Äôt necessarily the model that performs best on noisy data. To get a sense of how well your model might perform with noisy data, you can make small changes to your test splits to see how these changes affect your model‚Äôs performance. The more sensitive your model is to noise, the harder it will be to maintain it,since if your users‚Äô behaviors change just slightly, such as they change their phones, your model‚Äôs performance might degrade.Invariance tests Certain changes to the inputs shouldn‚Äôt lead to changes in the output. Discover biases: keep the inputs the same but change the sensitive information to see if the outputs change. Better, you should exclude the sensitive information from the features used to train the model in the first place. Directional expectation tests Certain changes to the inputs should, however, cause predictable changes in outputs. If the outputs change in the opposite expected direction, your model might not be learning the right thing, and you need to investigate it further before deploying it. Model calibration To quote Nate Silver in his book The Signal and the Noise, calibration is ‚Äúone of the most important tests of a forecast‚Äî I would argue that it is the single most important one. To measure a model‚Äôs calibration, a simple method is counting: you count the number of times your model outputs the probability X and the frequency Y of that prediction coming true, and plot X against Y. The graph for a perfectly calibrated model will have X equal Y at all data points. In scikit-learn, you can plot the calibration curve of a binary classifier with the method sklearn.calibration.calibration_curve Confidence measurement Confidence measurement can be considered a way to think about the usefulness threshold for each individual prediction. While most other metrics measure the system‚Äôs performance on average, confidence measurement is a metric for each individual sample. System-level measurement is useful to get a sense of overall performance, but sample-level metrics are crucial when you care about your system‚Äôs performance on every sample.1.2.3.Slice-based evaluation Slicing means to separate your data into subsets and look at your model‚Äôs perfor- mance on each subset separately. A common mistake that I‚Äôve seen in many compa- nies is that they are focused too much on coarse-grained metrics like overall F1 or accuracy on the entire data and not enough on sliced-based metrics. The focus on overall performance is harmful not only because of the potential public backlash, but also because it blinds the company to huge potential model improvements. A fascinating and seemingly counterintuitive reason why slice-based evaluation is crucial is Simpson‚Äôs paradox, a phenomenon in which a trend appears in several groups of data but disappears or reverses when the groups are combined. This means that model B can perform better than model A on all data together, but model A performs better than model B on each subgroup separately. Even when you don‚Äôt think slices matter, understanding how your model performs in a more fine-grained way can give you confidence in your model to convince other stakeholders, like your boss or your customers, to trust your ML models. To track your model‚Äôs performance on critical slices, you‚Äôd first need to know what your critical slices are. Heuristics-basedSlice your data using domain knowledge you have of the data and the task at hand. Error analysisManually go through misclassified examples and find patterns among them. Slice finderThe process generally starts with generating slice candidates with algorithms such as beam search, clustering, or decision, then pruning out clearly bad candidates for slices, and then ranking the candi- dates that are left. Chapter 7: Model deployment and prediction serviceTo be deployed, your model will have to leave the devel- opment environment. Your model can be deployed to a staging environment for testing or to aproduction environment to be used by your end users. If you want to deploy a model for your friends to play with, all you have to do is to wrap your predict function in a POST request endpoint using Flask or FastAPI, put the dependencies this predict function needs to run in a container,2 and push your model and its associated container to a cloud service like AWS or GCP to expose the endpoint: \n",
      "The hard parts include making your model available to millions of users with a latency of milliseconds and 99% uptime, setting up the infrastructure so that the right person can be immediately notified when something goes wrong, figuring out what went wrong, and seamlessly deploying the updates to fix what‚Äôs wrong. Exporting a model means converting this model into a format that can be used by another application. Some people call this process ‚Äúserialization. The process of generating predictions is called inference. Machine Learning Deployment Myths Myth 1: You Only Deploy One or Two ML Models at a Time In reality, companies have many, many ML models. An application might have many different features, and each feature might require its own model. Consider a ride-sharing app like Uber. It needs a model to predict each of the following elements: ride demand, driver availability, estimated time of arrival, dynamic pricing, fraudu- lent transaction, customer churn, and more. Additionally, if this app operates in 20 countries, until you can have models that generalize across different user-profiles, cultures, and languages, each country would need its own set of models. So with 20 countries and 10 models for each country, you already have 200 models. Myth 2: If We Don‚Äôt Do Anything, Model Performance Remains the Same ML systems suffer from what are known as data distribution shifts, when the data distribution your model encounters in production is different from the data‚óè‚óè‚óèdistribution it was trained on. Therefore, an ML model tends to perform best right after training and to degrade over time. Myth 3: You Won‚Äôt Need to Update Your Models as Much ‚ÄúHow often should I update my models?‚Äù It‚Äôs the wrong question to ask. The right question should be: ‚ÄúHow often can I update my models?‚Äù While many companies still only update their models once a month, or even once a quarter, Weibo‚Äôs iteration cycle for updating some of their ML models is 10 minutes. Myth 4: Most ML Engineers Don‚Äôt Need to Worry About Scale What ‚Äúscale‚Äù means varies from application to application, but examples include a system that serves hundreds of queries per second or millions of users a month. Batch Prediction Versus Online Prediction One fundamental decision you‚Äôll have to make that will affect both your end users and developers working on your system is how it generates and serves its predictions to end users: online or batch. Batch prediction, which uses only batch features. Online prediction that uses only batch features (e.g., precomputed embeddings). Online prediction that uses both batch features and streaming features. This is also known as streaming prediction. Online prediction is when predictions are generated and returned as soon as requests for these predictions arrive. = on-demand prediction // synchronous prediction Batch prediction is when predictions are generated periodically or whenever triggered. The predictions are stored somewhere, such as in SQL tables or an in-memory database, and retrieved as needed. = asynchronous prediction Features computed from historical data, such as data in databases and data warehouses, are batch features. Features computed from streaming data‚Äîdata in real-time transports‚Äîare‚óè‚óèstreaming features. In batch prediction, only batch features are used. In online prediction, however, it‚Äôs possible to use both batch features and streaming features. Online prediction and batch prediction don‚Äôt have to be mutually exclusive. One hybrid solution is that you precompute predictions for popular queries, then generate predictions online for less popular queries. From Batch Prediction to Online Prediction The more natural way to serve predictions is probably online. You give your model an input and it generates a prediction as soon as it receives that input. You export your model, upload the exported model to Amazon SageMaker or Google App Engine, and get back an exposed endpoint. Now, if you send a request that contains an input to that endpoint, it will send back a prediction generated on that input.A problem with online prediction is that your model might take too long to generate predictions. Instead of generating predictions as soon as they arrive, what if you compute predictions in advance and store them in your database, and fetch them when requests arrive? This is exactly what batch prediction does. Batch prediction can also be seen as a trick to reduce the inference latency of more complex models‚Äîthe time it takes to retrieve a prediction is usually less than the time it takes to generate it.  Batch prediction is good for when you want to generate a lot of predictions and don‚Äôt need the results immediately. The problem with batch prediction is that it makes your model less responsive to users‚Äô change preferences. Batch prediction is a workaround for when online prediction isn‚Äôt cheap enough or isn‚Äôt fast enough. Overcome the latency challenge of online prediction:A (near) real-time pipeline that can work with incoming data, extract streaming features (if needed), input them into a model, and return a prediction in near real-time. A streaming pipeline with real-time transport and a stream computation engine can help with that. A model that can generate predictions at a speed acceptable to its end users. For most consumer apps, this means milliseconds.‚Äì‚Äì‚ÄìHaving two different pipelines to process your data is a common cause for bugs in ML production. One cause for bugs is when the changes in one pipeline aren‚Äôt correctly replicated in the other, leading to two pipelines extracting two different sets of features. This is especially common if the two pipelines are maintained by two different teams.Building infrastructure to unify stream processing and batch processing has become a popular topic in recent years for the ML community. ‚Äî> using a stream processor like Apache Flink Model Compression If the model you want to deploy takes too long to generate predictions:make it do inference faster (= inference optimization)make the model smaller (= model compression)make the hardware it‚Äôs deployed on run faster ‚Äî> Cheng et al.‚Äôs ‚ÄúSurvey of Model Compression and Acceleration for Deep Neural Networks‚ÄúLow-Rank Factorization = replace high-dimensional tensors with lower-dimensional tensors One type of low-rank factorization is compact convolutional filters, where the over-parameterized (having too many parameters) convolution filters are replaced with compact blocks to both reduce the number of parameters and increase speed. Knowledge Distillation = a small model (student) is trained to mimic a larger model or ensemble of models (teacher) One example of a distilled network used in production is DistilBERT, which reduces the size of a BERT model by 40% while retaining 97% of its language understanding capabilities and being 60% faster advantage: it can work regardless of the architectural differences between the teacher and the student networks For example, you can get a random forest as the student and a transformer as the teacherdisadvantage: it‚Äôs highly dependent on the availability of a teacher network. Pruning 1= remove entire nodes of a neural network, which means changing its architecture and reducing its number of parameters 2= find parameters least useful to predictions and set them to 0 The architecture of the neural network remains the same. This helps with reducing the size of a model because pruning makes a neural network more sparse, and sparse architecture tends to require less storage space than dense structure. Quantization = reduces a model‚Äôs size by using fewer bits to represent its parameters. Also improves the computation speed. First, it allows us to increase our batch size. Second, less precision speeds up computation, which further reduces training time and inference latency. ML on the Cloud and on the Edge On the cloud means a large chunk of computation is done on the cloud, either public clouds or private clouds. On the edge means a large chunk of computation is done on consumer devices‚Äîsuch as browsers, phones, laptops, smartwatches, cars, security cameras, robots, embedded devices, FPGAs (field programmable gate arrays), and ASICs (application-specific integrated circuits)‚Äîwhich are also known as edge devices. easiest way: deploy via a managed cloud service (AWS, GCP)downside: cost - ML models can be compute-intensive, and computing is expensive. https://blog.tomilkieway.com/72k-1/Putting your models on the edge is also appealing when handling sensitive user data. Edge computing makes it easier to comply with regulations, like GDPR, about how user data can be transferred or stored.Because of the many benefits that edge computing has over cloud computing, companies are in a race to develop edge devices optimized for different ML usecases. Compiling and Optimizing Models for Edge Devices For a model built with a certain framework, such as TensorFlow or PyTorch, to run on a hardware backend, that framework has to be supported by the hardware vendor. IRs (intermediate representations) lie at the core of how compilers work. From the original code for a model, compilers generate a series of high- and low-level IRs before generating the code native to a hardware backend so that it can run on that hardware backend.\n",
      "This process is also called lowering, as in you ‚Äúlower‚Äù your high-level framework code into low-level hardware-native code. It‚Äôs not translating because there‚Äôs no one-to-one mapping between them. Model Optimization A typical ML workflow consists of many frameworks and libraries. For example, you might use pandas/dask/ray to extract features from your data. You might use NumPy to perform vectorization. You might use a pre-trained model like Hugging Face‚Äôs Transformers to generate features, then make predictions using an ensemble of models built with various frameworks like sklearn, TensorFlow, or LightGBM. In many companies, what usually happens is that data scientists and ML engineers develop models that seem to be working fine in development. However, when these models are deployed, they turn out to be too slow, so their companies hire optimization engineers to optimize their models for the1.2.3.4.hardware their models run on. There are two ways to optimize your ML models: locally and globally. Locally is when you optimize an operator or a set of operators of your model. Globally is when you optimize the entire computation graph end to end. Four common techniques for standard local optimization:VectorizationParallelizationLoop tilingOperator fusionUsing ML to optimize ML models There are a couple of drawbacks to hand-designed heuristics. First, they‚Äôre nonoptimal. There‚Äôs no guarantee that the heuristics an engineer comes up with are the best possible solution. Second, they are nonadaptive. Repeating the process on a new framework or a new hardware architecture requires an enormous amount of effort. If you don‚Äôt have ideas for good heuristics, one possible solution might be to try all possible ways to execute a computation graph, record the time they need to run, then pick the best one. ML in Browsers WASM (WebAssembly) is an open standard that allows you to run executable programs in browsers. After you‚Äôve built your models in scikit-learn, PyTorch, TensorFlow, or whatever frameworks you‚Äôve used, instead of compiling your models to run on specific hardware, you can compile your model to WASM. You get back an executable file that you can just use with JavaScript. main drawback of WASM is that because WASM runs in browsers, it‚Äôs slow.\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "loader = PyPDFLoader(\"./__original-data/üìöBOOK NOTE : Huyen_2022_Designing Machine Learning Systems: an iterative¬†‚Ä¶.pdf\")\n",
    "pages = loader.load_and_split()\n",
    "\n",
    "text = \"\"\n",
    "for page in pages:\n",
    "    text += page.page_content\n",
    "\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.embeddings import HuggingFaceInferenceAPIEmbeddings\n",
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "import tqdm as notebook_tqdm\n",
    "\n",
    "# EMBEDDINGS MODEL\n",
    "\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "hf_token = os.getenv(\"HF_EMBEDDINGS_MODEL\")\n",
    "embedding_url = \"https://api-inference.huggingface.co/pipeline/feature-extraction/sentence-transformers/all-MiniLM-L6-v2\"\n",
    "model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "\n",
    "HFembeddings = HuggingFaceInferenceAPIEmbeddings(\n",
    "    api_key=hf_token, model_name=model_name\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.2.3.4.5. üìöBOOK NOTE // Huyen_2022_Designing Machine Learning Systems: an iterative process for production-ready applications.Chapter 2: Introduction to Machine Learning Systems Design The ultimate goal of any project within a business is, therefore, to increase profits, either directly or indirectly: directly such as increasing sales (conversion rates) and cutting costs; indirectly such as higher customer satisfaction and increasing time spent on a website. What business performance metrics is the new ML system supposed to influence, e.g., the amount of ads revenue, the number of monthly active users? The longer you‚Äôve adopted ML, the more efficient your pipeline will run, the faster your development cycle will be, the less engineering time you‚Äôll need, and the lower your cloud bills will be, which all lead to higher returns. RequirementsReliability: With traditional software systems, you often get a warning, such as a system crash or runtime error or 404. However, ML systems can fail silently. Scalability: An ML system might grow in ML model count. Autoscaling is an indispensable feature in many cloud services: automatically scaling up and down the number of machines depending on usage. Maintainability: Code should be documented.\n"
     ]
    }
   ],
   "source": [
    "text_splitter = SemanticChunker(\n",
    "    HFembeddings, breakpoint_threshold_type=\"interquartile\"\n",
    "    )\n",
    "\n",
    "docs = text_splitter.create_documents([text])\n",
    "print(docs[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "  {\n",
      "    \"id\": 0,\n",
      "    \"chunk\": \"1.2.3.4.5. \\ud83d\\udcdaBOOK NOTE // Huyen_2022_Designing Machine Learning Systems: an iterative process for production-ready applications.Chapter 2: Introduction to Machine Learning Systems Design The ultimate goal of any project within a business is, therefore, to increase profits, either directly or indirectly: directly such as increasing sales (conversion rates) and cutting costs; indirectly such as higher customer satisfaction and increasing time spent on a website. What business performance metrics is the new ML system supposed to influence, e.g., the amount of ads revenue, the number of monthly active users? The longer you\\u2019ve adopted ML, the more efficient your pipeline will run, the faster your development cycle will be, the less engineering time you\\u2019ll need, and the lower your cloud bills will be, which all lead to higher returns. RequirementsReliability: With traditional software systems, you often get a warning, such as a system crash or runtime error or 404. However, ML systems can fail silently. Scalability: An ML system might grow in ML model count. Autoscaling is an indispensable feature in many cloud services: automatically scaling up and down the number of machines depending on usage. Maintainability: Code should be documented.\"\n",
      "  },\n",
      "  {\n",
      "    \"id\": 1,\n",
      "    \"chunk\": \"Code, data, and artifacts should be versioned. Models should be sufficiently reproducible so that even when the original authors are not around, other contributors can have sufficient contexts to build on their work. Adaptability: ML systems need to be able to evolve quickly. Iterative Process: Once a system is put into production, it\\u2019ll need to be continually monitored and updated. The process looks more like a cycle with a lot of back and forth between different steps. An ML problem is defined by inputs, outputs, and the objective function that guides the learning process. Classification models classify inputs into different categories. Regression models output a continuous value. A regression model can easily be framed as a classification model and vice versa.Within classification problems, the fewer classes there are to classify, the simpler the problem is. The simplest is binary classification, where there are only two possible classes. When there are more than two classes, the problem becomes multiclass classification.When the number of classes is high, we say the classification task has high cardinality.ML models typically need at least 100 examples for each class to learn to classify that class. So if you have 1,000 classes, you already need at least 100,000 examples. Solution: When the number of classes is large, hierarchical classification might be useful. In hierarchical classification, you have a classifier to first classify each example into one of the large groups. Then you have another classifier to classify this example into one of the subgroups. In both binary and multiclass classification, each example belongs to exactly one class. When an example can belong to multiple classes, we have a multilabel classification problem. To learn, an ML model needs an objective function to guide the learning process. An objective function is also called a loss function because the objective of the learning process is usually to minimize (or optimize) the loss caused by wrong predictions. Progress in the last decade shows that the success of an ML system depends largely on the data it was trained on.\"\n",
      "  },\n",
      "  {\n",
      "    \"id\": 2,\n",
      "    \"chunk\": \"Instead of focusing on improving ML algorithms, most companies focus on managing and improving their data. Mind might be disguised as inductive biases or intelligent architectural designs. Data might be grouped together with computation since more data tends to require more computation. Judea Pearl (The Book of Why): \\u201cData is profoundly dumb.\\u201d \\u201cML will not be the same in 3\\u20135 years, and ML folks who continue to follow the current data-centric paradigm will find themselves outdated, if not jobless. Take note.\\u201d Peter Norvig (Google director of search quality): \\u201cWe don\\u2019t have better algorithms. We just have more data.\\u201d Every project must start with why this project needs to happen, and ML projects are no exception.\\u25cf\\u25cf\\u25cf1.Chapter 3: Data Engineering FundamentalsIf data models describe the data in the real world, databases specify how the data should be stored on machines. Data SourcesUnderstanding the sources your data comes from can help you use your data more efficiently. One source is user input data, data explicitly input by users. User input data can be easily malformatted. System-generated data is the data generated by different components of your systems, which include various types of logs and system outputs such as model predictions. Because debugging ML systems is hard, it\\u2019s a common practice to log everything you can. To analyze logs: Logstash, Datadog, Logz.io, etc. Many of them use ML models to help you process and make sense of your massive number of logs. There are also internal databases, generated by various services and enterprise applications in a company. These databases manage their assets such as inventory, customer relationships, users, and more. First-party data is the data that your company already collects about your users or customers. Second-party data is the data collected by another company on their own customers that they make available to you, though you\\u2019ll probably have to pay for it. Third-party data companies collect data on the public who aren\\u2019t their direct customers. Data FormatsHow do I store multimodal data, e.g., a sample that might contain both images and texts? Where do I store my data so that it\\u2019s cheap and still fast to access?How do I store complex models so that they can be loaded and run correctly on different hardware? The process of converting a data structure or object state into a format that can be stored or transmitted and reconstructed later is data serialization. JSON = JavaScript Object Notation: Its key-value pair paradigm is simple but powerful, capable of handling data of different levels of1.2. 3.\"\n",
      "  },\n",
      "  {\n",
      "    \"id\": 3,\n",
      "    \"chunk\": \"4.structuredness. Row-Major Versus Column-Major Format: CSV (comma-separated values) is row-major, which means consecutive elements in a row are stored next to each other in memory. Parquet is column-major, which means consecutive elements in a column are stored next to each other.\\u2014> Because modern computers process sequential data more efficiently than nonsequential data, if a table is row-major, accessing its rows will be faster than accessing its columns in expectation. This means that for row-major formats, accessing data by rows is expected to be faster than accessing data by columns. NumPy Versus pandas- pandas is built around DataFrame, a concept inspired by R\\u2019s Data Frame, which is column-major. A DataFrame is a two-dimensional table with rows and columns. - In NumPy, the major order can be specified. When an ndarray is created, it\\u2019s row-major by default if you don\\u2019t specify the order. \\u2014> Accessing a DataFrame by row is so much slower than accessing the same DataFrame by column. Text Versus Binary Format: Text files are files that are in plain text, which usually means they are human-readable. Binary files are the catchall that refers to all nontext files. As the name suggests, binary files are typically files that contain only 0s and 1s, and are meant to be read or used by programs that know how to interpret the raw bytes.\\u2014> You want to store the number 1000000. If you store it in a text file, it\\u2019ll require 7 characters, and if each character is 1 byte, it\\u2019ll require 7 bytes.\"\n",
      "  },\n",
      "  {\n",
      "    \"id\": 4,\n",
      "    \"chunk\": \"If you store it in a binary file as int32, it\\u2019ll take only 32 bits or 4 bytes. => AWS recommends using the Parquet format because \\u201cthe Parquet format is up to 2x faster to unload and consumes up to 6x less storage in Amazon S3, compared to text formats.\\u201d Data ModelsData models describe how data is represented. How you choose to represent data not only affects the way your systems are built but also the problems your systems can solve.\"\n",
      "  },\n",
      "  {\n",
      "    \"id\": 5,\n",
      "    \"chunk\": \"Relational ModelsData is organized into relations; each relation is a set of tuples. A table is an accepted visual representation of a relation, and each row of a table makes up a tuple.One major downside of normalization is that your data is now spread across multiple relations. You can join the data from different relations back together, but joining can be expensive for large tables. Databases built around the relational data model are relational databases. The language that you can use to specify the data that you want from a database is called a query language. The most popular query language for relational databases today is SQL. SQL = declarative languageimperative paradigm = you specify the steps needed for an action and the computer executes these steps to return the outputs. declarative paradigm = you specify the outputs you want, and the computer figures out the steps needed to get you the queried outputs.\"\n",
      "  },\n",
      "  {\n",
      "    \"id\": 6,\n",
      "    \"chunk\": \"SQL can be Turing-complete, which means that, in theory, SQL can be used to solve any computation problem (without making any guarantee about the time or memory required). (https://wiki.postgresql.org/wiki/Cyclic_Tag_System)Declarative ML Systems:https://docs.h2o.ai/h2o/latest-stable/h2o-docs/automl.htmlhttps://github.com/ludwig-ai/ludwigNoSQLTwo major types of nonrelational models are the document model and the graph model. The document model targets use cases where data comes in self-contained documents and relationships between one document and another are rare. The graph model goes in the opposite direction, targeting use cases where relationships between data items are common and important. Compared to the relational model, it\\u2019s harder and less efficient to execute joins across documents compared to across tables.Structured Versus Unstructured Data Structured data follows a predefined data model, also known as a data schema. The disadvantage of structured data is that you have to commit your data to a predefined schema. If your schema changes, you\\u2019ll have to retrospectively update all your data, often causing mysterious bugs in the process. Unstructured data doesn\\u2019t adhere to a predefined data schema. It\\u2019s usually text but can also be numbers, dates, images, audio, etc. Unstructured data also allows for more flexible storage options. A repository for storing structured data is called a data warehouse.Data warehouses are used to store data that has been processed into formats ready to be used. A repository for storing unstructured data is called a data lake. Data lakes are usually used to store raw data before processing. Data Storage Engines and Processing Data formats and data models specify the interface for how users can store and retrieve data. Storage engines, also known as databases, are the implementation of how data is stored and retrieved on machines. Transactional and Analytical Processing The transactions are inserted as they are generated, and occasionally updated when something changes, or deleted when they are no longer needed (= online transaction processing (OLTP).Transactional databases are designed to process online transactions and satisfy the low latency, high availability requirements. Analytical questions require aggregating data in columns across multiple rows of data. Analytical databases are designed for this purpose (= online analytical processing (OLAP).ETL: Extract, Transform, and Load When data is extracted from different sources, it\\u2019s first transformed into the desired format before being loaded into the target destination such as a database or a data warehouse. In the extracting phase, you need to validate your data and reject the data that doesn\\u2019t meet your requirements. Transform is, where most of the data processing is done. You might want to join data from multiple sources and clean it. You might want to standardize the value ranges.Load is deciding how and how often to load your transformed data into the target destination, which can be a file, a database, or a data warehouse. Finding it difficult to keep data structured, some companies had this idea: \\u201cWhy not just store all data in a data lake so we don\\u2019t have to deal with schema changes? This process of loading data into storage first and then processing it later is sometimes called ELT (extract, load, transform).\\u25cf\\u25cf\\u25cb\\u25cb\\u25cb\\u25cb\\u25cf\\u25cb\\u25cb\\u25cbBUT: It\\u2019s inefficient to search through a massive amount of raw data for the data that you want. BUT 2: As companies switch to running applications on the cloud and infrastructures become standardized, data structures also become standardized. Modes of Dataflow How do we pass data between different processes that don\\u2019t share memory? When data is passed from one process to another, we say that the data flows from one process to another, which gives us a dataflow. Data passing through databases Data passing through services using requests such as the requests provided by REST and RPC APIs (e.g., POST/GET requests)Because processes communicate through requests, we say that this is request-driven.The most popular styles of requests used for passing data through networks are REST (representational state transfer) and RPC (remote procedure call) Implementations of a REST architecture are said to be RESTful.REST doesn\\u2019t exactly mean HTTP because HTTP is just an implementation of REST. Data passing through a real-time transport like Apache Kafka and Amazon Kinesis each service can write data to a database and other services that need the data can read from that database. A piece of data broadcast to a real-time transport is called an event. This architecture is, therefore, also called event-driven. pubsub (publish-subscribe) model = any service can publish to different topics in a real-time transport, and any service that subscribes to a topic can read all the events in that topic. The services that produce data don\\u2019t care about what services consume their data. Batch Processing Versus Stream Processing Once your data arrives in data storage engines like databases, data lakes, or data warehouses, it becomes historical data. Historical data is often processed in batch jobs\\u2014jobs that are kicked off periodically.\\u2013When data is processed in batch jobs, we refer to it as batch processing. When you have data in real-time transports like Apache Kafka and Amazon Kinesis, we say that you have streaming data. Stream processing refers to doing computation on streaming data. Stream processing, when done right, can give low latency because you can process data as soon as data is generated, without having to first write it into databases. Batch features\\u2014features extracted through batch processing\\u2014are also known as static features. Streaming features\\u2014 features extracted through stream processing\\u2014are also known as dynamic features. Chapter 4: Training DataData is full of potential biases.\"\n",
      "  },\n",
      "  {\n",
      "    \"id\": 7,\n",
      "    \"chunk\": \"These biases have many possible causes. There are biases caused during collecting, sampling, or labeling. Historical data might be embedded with human biases, and ML models, trained on this data, can perpetuate them. Use data but don\\u2019t trust it too much! SamplingSampling happens in many steps of an ML project lifecycle, such as sampling from all possible real-world data to create training data; sampling from a given dataset to create splits for training, validation, and testing, etc.Nonprobability Sampling Nonprobability sampling is when the selection of data isn\\u2019t based on any probability criteria. Beispiel: Convenience sampling Samples of data are selected based on their availability. This sampling method is popular because, well, it\\u2019s convenient. The samples selected by nonprobability criteria are not representative of the real-world data and therefore are riddled with selection biases. Language models are often trained not with data that is representative of all possible texts but with data that can be easily collected\\u2014Wikipedia, Common Crawl, Reddit. Simple Random SamplingIn the simplest form of random sampling, you give all samples in the population equal probabilities of being selected. For example, you randomly select 10% of the population, giving all members of this population an equal 10% chance of being selected. Stratified Sampling To avoid the drawback of simple random sampling, you can first divide your population into the groups that you care about and sample from each group separately. Weighted Sampling Each sample is given a weight, which determines the probability of it being selected. This method allows you to leverage domain expertise. Reservoir Sampling One solution for this problem is reservoir sampling. The algorithm involves a reservoir, which can be an array.Importance Sampling It allows us to sample from a distribution when we only have access to another distribution. Labeling The performance of an ML model still depends heavily on the quality and quantity of the labeled data it\\u2019s trained on. Hand Labels Hand-labeling data can be expensive, especially if subject matter expertise is required. Hand labeling poses a threat to data privacy Hand labeling is slow Slow labeling leads to slow iteration speed and makes your model less adaptive to changing environments and requirements. If the task changes or data changes, you\\u2019ll have to wait for your data to be relabeled before updating your model.Label multiplicity To obtain enough labeled data, companies have to use data from multiple sources and rely on multiple annotators who have different levels of expertise. These different data sources and annotators also have different levels of accuracy. Data lineage It\\u2019s good practice to keep track of the origin of each of your data samples as well as its labels, a technique known as data lineage. Data lineage helps you both flag potential biases in your data and debug your models. Natural Labels Tasks with natural labels are tasks where the model\\u2019s predictions can be automatically evaluated or partially evaluated by the system. Semi-supervision A classic semi-supervision method is self-training. You start by training a model on your existing set of labeled data and use this model to make predictions for unlabeled samples. Assuming that predictions with high raw probability scores are correct, you add the labels predicted with high probability to your training set and train a new model on this expanded training set. This goes on until you\\u2019re happy with your model performance. Semi-supervision is the most useful when the number of training labels is limited. Transfer learning Transfer learning refers to the family of methods where a model developed for a task is reused as the starting point for a model on a second task. Transfer learning is especially appealing for tasks that don\\u2019t have a lot of labeled data. Active learning Active learning is a method for improving the efficiency of data labels. Instead of randomly labeling data samples, you label the samples that are most help- ful to your models according to some metrics or heuristics.Class Imbalance Class imbalance typically refers to a problem in classification tasks where there is a substantial difference in the number of samples in each class of the training data. ML, especially deep learning, works well in situations when the data distribution is more balanced, and usually not so well when the classes are heavily imbalanced Precision, Recall, and F1 true positive rate (also known as recall) false positive rate (also known as the probability of false alarm) We can plot the true positive rate against the false positive rate for different thresholds. This plot is known as the ROC curve (receiver operating characteristics) \\u2014> When your model is perfect, the recall is 1.0, and the curve is just a line at the top The area under the curve (AUC) measures the area under the ROC curve. Since the closer to the perfect line the better, the larger this area the better Data-level methods: Resampling Data-level methods modify the distribution of the training data to reduce the level of imbalance to make it easier for the model to learn. A common family of techniques is resampling. Resampling includes oversampling, adding more instances from the minority classes, and undersampling, removing instances of the majority classes. When you resample your training data, never evaluate your model on resampled data, since it will cause your model to overfit to that resampled distribution. Undersampling runs the risk of losing important data from removing data. Data Augmentation Data augmentation is a family of techniques that are used to increase the amount of training data.In computer vision, the simplest data augmentation technique is to randomly modify an image while preserving its label. In NLP, you can randomly replace a word with a similar word, assuming that this replacement wouldn\\u2019t change the meaning or the sentiment of the sentence.Perturbation (St\\u00f6rung)Perturbation is also a label-preserving operation Using deceptive data to trick a neural network into making wrong predictions is called adversarial attacks. Adding noise to samples is a common technique to create adversarial samples. Data Synthesis Since collecting data is expensive and slow, with many potential privacy concerns, it\\u2019d be a dream if we could sidestep it altogether and train our models with synthesized data. Even though we\\u2019re still far from being able to synthesize all training data, it\\u2019s possible to synthesize some training data to boost a model\\u2019s performance. Chapter 5: Feature EngineeringState-of-the-art model architectures can still perform poorly if they don\\u2019t use a good set of features. A large part of many ML engineering and data science jobs is to come up with new useful features. Learned Features Versus Engineered Features The promise of deep learning is that we won\\u2019t have to handcraft features. For this reason, deep learning is sometimes called feature learning. Many features can be automatically learned and extracted by algorithms. However, we\\u2019re still far from the point where all features can be automated. EXKURS n-gram: an n-gram is a contiguous sequence of n items from a given sample of text. The items can be phonemes, syllables, letters, or words. For example, given the post \\u201cI like food,\\u201d its word-level 1-grams are [\\u201cI\\u201d , \\u201clike\\u201d , \\u201cfood\\u201d] and its word-level 2-grams are [\\u201cI like\\u201d , \\u201clike food\\u201d].Once you\\u2019ve generated n-grams for your training data, you can create a vocabulary that maps each n-gram to an index. Then you can convert each post into a vector based on its n-grams\\u2019 indices. Feature engineering requires knowledge of domain-specific techniques\\u2014in this case, the domain is natural language processing (NLP) and the native language of the text. With Deep Learning: Instead of having to worry about lemmatization, punctuation, or stopword removal, you can just split your raw text into words (i.e., tokenization), create a vocabulary out of those words, and convert each of your words into one-shot vectors using this vocabulary. The process of choosing what information to use and how to extract this information into a format usable by your ML models is feature engineering.\"\n",
      "  },\n",
      "  {\n",
      "    \"id\": 8,\n",
      "    \"chunk\": \"For complex tasks such as recommending videos for users to watch next on TikTok, the number of features used can go up to millions.For domain-specific tasks, you might need subject matter expertise. Common Feature Engineering Operations Handling Missing Values Missing not at random (MNAR) This is when the reason a value is missing is because of the true value itself. Missing at random (MAR)This is when the reason a value is missing is not due to the value itself, but due to another observed variable. Missing completely at random (MCAR)This is when there\\u2019s no pattern in when the value is missing. This type of missing is very rare. There are usually reasons why certain values are missing, and you should investigate. Solution:One way to delete is column deletion: if a variable has too many missing values, just remove that variable. Another way to delete is row deletion: if a sample has missing value(s), just remove that sample. However, removing rows of data can also remove important information thatyour model needs to make predictions (especially when MNAR)On top of that, removing rows of data can create biases in your model (especially when MAR)If you don\\u2019t want to delete missing values, you will have to impute them, which means \\u201cfill them with certain values.\\u201d One common practice is to fill in missing values with their defaults (or with median, mean or mode - most common value).You want to avoid filling missing values with possible values.Multiple techniques might be used at the same time or in sequence to handle missing values for a particular set of data. \\u2014> There is no perfect way to handle missing values Scaling Before inputting features into models, it\\u2019s important to scale them to similar ranges. This process is called feature scaling.\"\n",
      "  },\n",
      "  {\n",
      "    \"id\": 9,\n",
      "    \"chunk\": \"This is one of the simplest things you can do that often results in a performance boost for your model. An intuitive way to scale your features is to get them to be in the range [0, 1]. If you think that your variables might follow a normal distribution, it might be helpful to normalize them so that they have zero mean and unit variance (standardization).In many cases, the log transformation can help reduce the skewness of your data.Encoding Categorical Features In production, categories change. One solution to this problem (dynamic change in categories) is the hashing trick.The gist of this trick is that you use a hash function to generate a hashed value of each category.\"\n",
      "  },\n",
      "  {\n",
      "    \"id\": 10,\n",
      "    \"chunk\": \"The hashed value will become the index of that category. Because you can specify the hash space, you can fix the number of encoded values for a feature in advance, without having to know how many categoriesthere will be. One problem with hashed functions is collision: two categories being assigned the same index. You can choose a hash space large enough to reduce the collision. Feature Crossing Feature crossing is the technique to combine two or more features to generate new features. This technique is useful to model the nonlinear relationships between features. It\\u2019s less important in neural networks, but it can still be useful because explicit feature crossing occasionally helps neural networks learn nonlinear relationships faster. Discrete and Continuous Positional Embeddings Consider the task of language modeling where you want to predict the next token (e.g., a word, character, or subword) based on the previous sequence of tokens. EXKURS embeddings: An embedding is a vector that represents a piece of data. We call the set of all possible embeddings generated by the same algorithm for a type of data \\u201can embedding space.\\u201d All embedding vectors in the same space are of the same size. The embedding size for positions is usually the same as the embedding size for words so that they can be summed. Because the embeddings change as the model weights get updated, we say that the position embeddings are learned. The embedding for each position is still a vector with S elements (S is the position embedding size), but each element is predefined using a function, usually sine and cosine. In the original Transformer paper, if the element is at an even index, use sine. Else, use cosine.\"\n",
      "  },\n",
      "  {\n",
      "    \"id\": 11,\n",
      "    \"chunk\": \"Fixed positional embedding is a special case of what is known as Fourier features. If positions in positional embeddings are discrete, Fourier features can also be continuous. Data LeakageData leakage refers to the phenomenon when a form of the label \\u201cleaks\\u201d into the set of features used for making predictions, and this same information is not available during inference. It\\u2019s dangerous because it can cause your models to fail in an unexpected and spectacular way Splitting time-correlated data randomly instead of by time In many cases, data is time-correlated, which means that the time the data is generated affects its label distribution. To prevent future information from leaking into the training process and allowing models to cheat during evaluation, split your data by time, instead of splitting randomly, whenever possible. For example, if you have data from five weeks, use the first four weeks for the train split, then randomly split week 5 into validation and test.Scaling before splitting One common mistake is to use the entire training data to generate global statistics before splitting it into different splits, leaking the mean and variance of the test samples into the training process, and allowing a model to adjust its predictions for the test samples. To avoid this type of leakage, always split your data first before scaling, then use the statistics from the train split to scale all the splits. Filling in missing data with statistics from the test split Leakage might occur if the mean or median is calculated using the entire data instead of just the train split. It can be prevented by using only statistics from the train split to fill in missing values in all the splits. Poor handling of data duplication before splitting If you have duplicates or near-duplicates in your data, failing to remove them before splitting your data might cause the same samples to appear in both train and validation/test splits. Data duplication can result from data collection or merging of different data sources.To avoid this, always check for duplicates before splitting and also after splitting just to make sure. If you oversample your data, do it after splitting. Group leakage A group of examples have strongly correlated labels but are divided into different splits. his type of leakage is common for objective detection tasks that contain photos of the same object taken milliseconds apart\\u2014some of them landed in the train split while others landed in the test split. It\\u2019s hard avoiding this type of data leakage without understanding how your data was generated. Leakage from data generation process Detecting this type of data leakage requires a deep understanding of the way data is collected. There\\u2019s no foolproof way to avoid this type of leakage, but you can mitigate the risk by keeping track of the sources of your data and understanding how it is collected and processed. Normalize your data so that data from different sources can have the same means and variances. Detecting Data Leakage Measure the predictive power of each feature or a set of features with respect to the target variable (label). If a feature has an unusually high correlation, investigate how this feature is generated and whether the correlation makes sense. Do ablation studies to measure how important a feature or a set of features is to your model. If removing a feature causes the model\\u2019s performance to deteriorate significantly, investigate why that feature is so important. Keep an eye out for new features added to your model. If adding a new feature significantly improves your model\\u2019s performance, either that feature is really good or that feature just contains leaked information about labels. Be very careful every time you look at the test split.If you use the test split in any way other than to report a model\\u2019s final performance, you risk leaking information from the future into your training process. Engineering Good Features Generally, adding more features leads to better model performance. If a feature doesn\\u2019t help a model make good predictions, regularization techniques like L1 regularization should reduce that feature\\u2019s weight to 0. Feature Importance If you use a classical ML algorithm like boosted gradient trees, the easiest way to measure the importance of your features is to use built-in feature importance functions implemented by XGBoost. For more model-agnostic methods, you might want to look into SHAP (SHapley Additive exPlanations).SHAP is great because it not only measures a feature\\u2019s importance to an entire model, but it also measures each feature\\u2019s contribution to a model\\u2019s specific prediction. InterpretML: https://github.com/interpretml/interpretOften, a small number of features accounts for a large portion of your model\\u2019s feature importance. Feature Generalization Not all features generalize equally.\"\n",
      "  },\n",
      "  {\n",
      "    \"id\": 12,\n",
      "    \"chunk\": \"Measuring feature generalization is a lot less scientific than measuring feature importance, and it requires both intuition and subject matter expertise on top of statistical knowledge. Coverage is the percentage of the samples that has values for this feature in the data\\u2014 so the fewer values that are missing, the higher the coverage. feature distribution: If the set of values that appears in the seen data (such as the train split) has no overlap with the set of values that appears in the unseen data (such as the test split), this feature might even hurt your model\\u2019s performanceChapter 6: Model development and Offline EvaluationIt allows to play around with different algorithms and techniques, even the latest ones.To build an ML model, we first need to select the ML model to build. Model development is an iterative process.\"\n",
      "  },\n",
      "  {\n",
      "    \"id\": 13,\n",
      "    \"chunk\": \"After each iteration, you\\u2019ll want to com- pare your model\\u2019s performance against its performance in previous iterations and evaluate how suitable this iteration is for production. Model EvaluationIf you had unlimited time and compute power, the rational thing to do would be to try all possible solutions and see what is best for you. However, time and compute power are limited resources, and you have to be strategic about what models you select. However, even though deep learning is finding more use cases in production, classical ML algorithms are not going away. Many recommender systems still rely on collaborative filtering and matrix factorization. Tree-based algorithms, including gradient-boosted trees, still power many classifica- tion tasks with strict latency requirements. Example: A k-means clustering model might be used to extract features to input into a neural network.\"\n",
      "  },\n",
      "  {\n",
      "    \"id\": 14,\n",
      "    \"chunk\": \"When selecting a model for your problem, you don\\u2019t choose from every possible model out there, but usually focus on a set of models suitable for your problem. Example: If your boss tells you to build a system to detect toxic tweets, you know that this is a text classification problem\\u2014given a piece of text, classify whether it\\u2019s toxic or not\\u2014and common models for text classification include naive Bayes, logistic regression, recurrent neural networks, and transformer-based models such as BERT, GPT, and their variants. Knowledge of common ML tasks and the typical approaches to solve them is essential in this process. Different types of algorithms require different numbers of labels as well as different amounts of compute power. When considering what model to use, it\\u2019s important to consider not only the model\\u2019s performance, measured by metrics such as accuracy, F1 score, and log loss, but also its other properties, such as how much data, compute, and time it needs to train, what\\u2019s its inference latency, and interpretability.1.2. 3.\"\n",
      "  },\n",
      "  {\n",
      "    \"id\": 15,\n",
      "    \"chunk\": \"4. 5.To understand different algorithms, the best way is to equip yourself with basic ML knowledge and run experiments with the algorithms you\\u2019re interested in. Important: Follow major ML conferences such as NeurIPS, ICLR, and ICML, as well as following researchers whose work has a high signal-to-noise ratio on Twitter. Six tips for model selection Avoid the state-of-the-art trap\\u2014> What means \\u201estate-of-the-art\\u201c?Being state-of-the-art often means that it performs better than existing models on some static datasets. It doesn\\u2019t mean that this model will be fast enough or cheap enough for you to implement.If there\\u2019s a solution that can solve your problem that is much cheaper and simpler than state-of-the-art models, use the simpler solution. Start with the simplest models Zen of Python states that \\u201csimple is better than complex,\\u201d 1. simpler models are easier to deploy 2.\"\n",
      "  },\n",
      "  {\n",
      "    \"id\": 16,\n",
      "    \"chunk\": \"easier to understand your model 3. simplest model serves as a baseline \\u2014> pretrained BERT models are complex, but they require little effort to get started with, especially if you use a ready-made implementation like the one in Hugging Face\\u2019s Transformer. Avoid human biases in selecting models Part of the process of evaluating an ML architecture is to experiment with different features and different sets of hyperparameters to find the best model of that architecture. If an engineer is more excited about an architecture, they will likely spend a lot more time experimenting with it, which might result in better-performing models for that architecture. When comparing different architectures, it\\u2019s important to compare them under comparable setups. Evaluate good performance now versus good performance laterA simple way to estimate how your model\\u2019s performance might change with more data is to use learning curves Learning Curve: a plot of its performance\\u2014e.g., training loss, training accuracy, validation accuracy\\u2014against the number of training samples it uses While evaluating models, you might want to take into account their potential for improvements in the near future, and how easy/difficult it is to achieve those improvements. Evaluate trade-offs A more complex model can give a better performance, but its results5.6. 1.\\u2013\\u2013\\u2013\\u20131.\\u2013\\u2013\\u20131.\\u2013\\u2013are less interpretable. Understand your model\\u2019s assumptions The real world is intractably complex, and models can only approximate using assumptions. Ensembles One method that has consistently given a performance boost is to use an ensemble of multiple models instead of just an individual model to make predictions. Each model in the ensemble is called a base learner. Ensembling methods are less favored in production because ensembles are more complex to deploy and harder to maintain. When creating an ensemble, the less correlation there is among base learners, the better the ensemble will be. Therefore, it\\u2019s common to choose very different types of models for an ensemble. Bagging = bootstrap aggregating designed to improve both the training stability and accuracy of ML algorithms. reduces variance and helps to avoid overfitting. Given a dataset, instead of training one classifier on the entire dataset, you sample with replacement to create different datasets, called bootstraps, and train a classification or regression model on each of these bootstraps. Why Sampling? Sampling with replacement ensures that each bootstrap is created independently from its peers. Boosting = a family of iterative ensemble algorithms that convert weak learners to strong ones Each learner in this ensemble is trained on the same set of samples, but the samples are weighted differently among iterations. future weak learners focus more on the examples that previous weak learners misclassified. XGBoost, a variant of GBM, used to be the algorithm of choice for many winning teams in ML competitions. Stacking you train base learners from the training data and then create a meta-learner that combines the outputs of the base learners to output final predictions meta-learner can be as simple as a heuristic: you take the majority\\u2013vote (for classification tasks) or the average vote (for regression tasks) from all base learners. Experiment Tracking and Versioning It\\u2019s important to keep track of all the definitions needed to re-create an experiment and its relevant artifacts. An artifact is a file generated during an experiment\\u2014examples of artifacts can be files that show the loss curve, evaluation loss graph, etc.The process of tracking the progress and results of an experiment is called experiment tracking. The process of logging all the details of an experiment for the purpose of possibly recreating it later or comparing it with other experiments is called versioning. Experiment tracking = babysitting the learning processes It\\u2019s important to track what\\u2019s going on during training not only to detect and address these issues but also to evaluate whether your model is learning anything useful. What should/could be tracked:Loss curve, model performance metrics, corresponding sample, prediction, ground truth label, speed of model, system performance metrics, parameter, hyperparameterA simple way to track your experiments is to automatically make copies of all the code files needed for an experiment and log all outputs with their timestamps. Versioning ML systems are part code, part data, so you need to not only version your code but your data as well. Code versioning tools allow users to revert to a previous version of the codebase by keeping copies of all the old files. Debugging ML Models ML models fail silently\\u2013\\u2013\\u2013\\u2013\\u2013\\u2013\\u2013\\u2013even when you think you\\u2019ve found the bug, it can be frustratingly slow to validate whether the bug has been fixed debugging ML models is hard because of their cross-functional complexity. There are many components in an ML system: data, labels, features, ML algorithms, code, infrastructure, etc. These different components might be owned by different teams. some of the things that might cause an ML model to fail: Theoretical constraints Poor implementation of model Poor choice of hyperparameters Data problems Poor choice of features tried-and-true debugging techniques published by experienced ML engineers and researchers Start simple and gradually add more components Overfit a single batch Set a random seed Distributed Training It\\u2019s common to train a model using data that doesn\\u2019t fit into memory. When a sample of your data is large, e.g., one machine can handle a few samples at a time, you might only be able to work with a small batch size, which leads to instability for gradient descent-based optimization. In some cases, a data sample is so large it can\\u2019t even fit into memory and you will have to use something like gradient checkpointing, a technique that leverages the memory footprint and compute trade-off to make your system do more computation with less memory. Data parallelism you split your data on multiple machines, train your model on all of them, and accumulate gradients. Model parallelism Model parallelism is when different components of your model are trained on different machinesModel parallelism can be misleading because in some cases parallelism doesn\\u2019t mean that different parts of the model in different machines are executed in parallel. Pipeline parallelism is a clever technique to make different components of a model on different machines run more in parallel. When machine 1 finishes the first part of its computation, it passes the result onto machine 2, then continues to the second part, and so on. AutoML AutoML refers to automating the process of finding ML algorithms to solve real-world problems. One mild form, and the most popular form, of AutoML in production is hyperparameter tuning. hyperparameter = parameter supplied by users whose value is used to control the learning process, e.g., learning rate, batch size, number of hidden layers, number of hidden units, dropout probability, \\u03b21 and \\u03b22 in Adam optimizer, etc. With different sets of hyperparameters, the same model can give drastically different performances on the same dataset. Melis et al. showed in their 2018 paper \\u201cOn the State of the Art of Evaluation in Neural Language Models\\u201d that weaker models with well-tuned hyperparameters can outperform stronger, fancier models. The goal of hyperparameter tuning is to find the optimal set of hyperparameters for a given model within a search space Popular ML frameworks either come with built-in utilities or have third-party utilities for hyperparameter tuning\\u2014for example, scikit-learn with auto-sklearn,25 TensorFlow with Keras Tuner, and Ray with Tune. Popular methods for hyperparameter tuning include random search,26 grid search, and Bayesian optimization. When tuning hyperparameters, keep in mind that a model\\u2019s performance might be more sensitive to the change in one hyperparameter than another, and therefore sensitive hyperparameters should be more carefully tuned. It\\u2019s crucial to never use your test split to tune hyperparameters. Choose the best set of hyperparameters for a model based on its performance on a validation split, then report the model\\u2019s final performance on the test split. If you use your test split to tune hyperparameters, you risk overfitting your1.2.3.4.model to the test split. Hard AutoMLInstead of manually putting a pooling layer after a convolutional layer or ReLu (rectified linear unit) after linear, you give your algorithm these building blocks and let it figure out how to combine them. = neural architecture search (NAS) Four Phases of ML Model Development Before machine learning If this is your first time trying to make this type of prediction from this type of data, start with non-ML solutions. Your first stab at the problem can be the simplest heuristics. Simplest machine learning models you want to start with a simple algorithm, something that gives you visibility into its working to allow you to validate the usefulness of your problem framing and your data. \\u2014> Logistic regression, gradient-boosted trees, k-nearest neighborOptimizing simple models different objective functions, hyperparameter search, feature engineering, more data, and ensembles Complex models experiment with more complex models Model Offline Evaluation How do I know that our ML models are any good? the evaluation methods should be the same during both development and production. But in many cases, the ideal is impossible because you have ground truth labels during development, but in production, you don\\u2019t. it\\u2019s possible to infer or approximate labels in production based on users\\u2019 feedback For other tasks, you might not be able to evaluate your model\\u2019s performance in production directly and might have to rely on extensive monitoring to detect changes and failures in your ML system\\u2019s performance. Baselines\\u25cf1.2.3.4.5.EXKURS: FID score a common metric for measuring the quality of synthesized images. The smaller the value, the higher the quality is supposed to be.\"\n",
      "  },\n",
      "  {\n",
      "    \"id\": 17,\n",
      "    \"chunk\": \"Evaluation metrics, by themselves, mean little. When evaluating your model, it\\u2019s essential to know the baseline you\\u2019re evaluating it against. Five essential baselines, that might be useful:Random baselineIf our model just predicts at random, what\\u2019s the expected performance? Simple heuristic Forget ML. If you just make predictions based on simple heuristics, what performance would you expect? Zero rule baseline The zero rule baseline is a special case of the simple heuristic baseline when your baseline model always predicts the most common class. Human baseline In many cases, the goal of ML is to automate what would have been otherwise done by humans, so it\\u2019s useful to know how your model performs compared to human experts. Existing solutions ML systems are designed to replace existing solutions, which might be business logic with a lot of if/else statements or third-party solutions. It\\u2019s crucial to compare your new model to these existing solutions.\"\n",
      "  },\n",
      "  {\n",
      "    \"id\": 18,\n",
      "    \"chunk\": \"When evaluating a model, it\\u2019s important to differentiate between \\u201ca good system\\u201d and \\u201ca useful system.\\u201d A good system isn\\u2019t necessarily useful, and a bad system isn\\u2019t necessarily useless. Evaluation Methods Perturbation tests The model that performs best on training data isn\\u2019t necessarily the model that performs best on noisy data. To get a sense of how well your model might perform with noisy data, you can make small changes to your test splits to see how these changes affect your model\\u2019s performance. The more sensitive your model is to noise, the harder it will be to maintain it,since if your users\\u2019 behaviors change just slightly, such as they change their phones, your model\\u2019s performance might degrade.Invariance tests Certain changes to the inputs shouldn\\u2019t lead to changes in the output. Discover biases: keep the inputs the same but change the sensitive information to see if the outputs change. Better, you should exclude the sensitive information from the features used to train the model in the first place. Directional expectation tests Certain changes to the inputs should, however, cause predictable changes in outputs. If the outputs change in the opposite expected direction, your model might not be learning the right thing, and you need to investigate it further before deploying it. Model calibration To quote Nate Silver in his book The Signal and the Noise, calibration is \\u201cone of the most important tests of a forecast\\u2014 I would argue that it is the single most important one. To measure a model\\u2019s calibration, a simple method is counting: you count the number of times your model outputs the probability X and the frequency Y of that prediction coming true, and plot X against Y. The graph for a perfectly calibrated model will have X equal Y at all data points. In scikit-learn, you can plot the calibration curve of a binary classifier with the method sklearn.calibration.calibration_curve Confidence measurement Confidence measurement can be considered a way to think about the usefulness threshold for each individual prediction. While most other metrics measure the system\\u2019s performance on average, confidence measurement is a metric for each individual sample. System-level measurement is useful to get a sense of overall performance, but sample-level metrics are crucial when you care about your system\\u2019s performance on every sample.1.2.3.Slice-based evaluation Slicing means to separate your data into subsets and look at your model\\u2019s perfor- mance on each subset separately. A common mistake that I\\u2019ve seen in many compa- nies is that they are focused too much on coarse-grained metrics like overall F1 or accuracy on the entire data and not enough on sliced-based metrics. The focus on overall performance is harmful not only because of the potential public backlash, but also because it blinds the company to huge potential model improvements. A fascinating and seemingly counterintuitive reason why slice-based evaluation is crucial is Simpson\\u2019s paradox, a phenomenon in which a trend appears in several groups of data but disappears or reverses when the groups are combined. This means that model B can perform better than model A on all data together, but model A performs better than model B on each subgroup separately. Even when you don\\u2019t think slices matter, understanding how your model performs in a more fine-grained way can give you confidence in your model to convince other stakeholders, like your boss or your customers, to trust your ML models. To track your model\\u2019s performance on critical slices, you\\u2019d first need to know what your critical slices are. Heuristics-basedSlice your data using domain knowledge you have of the data and the task at hand. Error analysisManually go through misclassified examples and find patterns among them. Slice finderThe process generally starts with generating slice candidates with algorithms such as beam search, clustering, or decision, then pruning out clearly bad candidates for slices, and then ranking the candi- dates that are left. Chapter 7: Model deployment and prediction serviceTo be deployed, your model will have to leave the devel- opment environment.\"\n",
      "  },\n",
      "  {\n",
      "    \"id\": 19,\n",
      "    \"chunk\": \"Your model can be deployed to a staging environment for testing or to aproduction environment to be used by your end users. If you want to deploy a model for your friends to play with, all you have to do is to wrap your predict function in a POST request endpoint using Flask or FastAPI, put the dependencies this predict function needs to run in a container,2 and push your model and its associated container to a cloud service like AWS or GCP to expose the endpoint: \\nThe hard parts include making your model available to millions of users with a latency of milliseconds and 99% uptime, setting up the infrastructure so that the right person can be immediately notified when something goes wrong, figuring out what went wrong, and seamlessly deploying the updates to fix what\\u2019s wrong. Exporting a model means converting this model into a format that can be used by another application. Some people call this process \\u201cserialization. The process of generating predictions is called inference. Machine Learning Deployment Myths Myth 1: You Only Deploy One or Two ML Models at a Time In reality, companies have many, many ML models. An application might have many different features, and each feature might require its own model. Consider a ride-sharing app like Uber. It needs a model to predict each of the following elements: ride demand, driver availability, estimated time of arrival, dynamic pricing, fraudu- lent transaction, customer churn, and more. Additionally, if this app operates in 20 countries, until you can have models that generalize across different user-profiles, cultures, and languages, each country would need its own set of models. So with 20 countries and 10 models for each country, you already have 200 models. Myth 2: If We Don\\u2019t Do Anything, Model Performance Remains the Same ML systems suffer from what are known as data distribution shifts, when the data distribution your model encounters in production is different from the data\\u25cf\\u25cf\\u25cfdistribution it was trained on.\"\n",
      "  },\n",
      "  {\n",
      "    \"id\": 20,\n",
      "    \"chunk\": \"Therefore, an ML model tends to perform best right after training and to degrade over time. Myth 3: You Won\\u2019t Need to Update Your Models as Much \\u201cHow often should I update my models?\\u201d It\\u2019s the wrong question to ask. The right question should be: \\u201cHow often can I update my models?\\u201d While many companies still only update their models once a month, or even once a quarter, Weibo\\u2019s iteration cycle for updating some of their ML models is 10 minutes. Myth 4: Most ML Engineers Don\\u2019t Need to Worry About Scale What \\u201cscale\\u201d means varies from application to application, but examples include a system that serves hundreds of queries per second or millions of users a month. Batch Prediction Versus Online Prediction One fundamental decision you\\u2019ll have to make that will affect both your end users and developers working on your system is how it generates and serves its predictions to end users: online or batch. Batch prediction, which uses only batch features. Online prediction that uses only batch features (e.g., precomputed embeddings). Online prediction that uses both batch features and streaming features. This is also known as streaming prediction. Online prediction is when predictions are generated and returned as soon as requests for these predictions arrive. = on-demand prediction // synchronous prediction Batch prediction is when predictions are generated periodically or whenever triggered. The predictions are stored somewhere, such as in SQL tables or an in-memory database, and retrieved as needed. = asynchronous prediction Features computed from historical data, such as data in databases and data warehouses, are batch features. Features computed from streaming data\\u2014data in real-time transports\\u2014are\\u25cf\\u25cfstreaming features. In batch prediction, only batch features are used. In online prediction, however, it\\u2019s possible to use both batch features and streaming features. Online prediction and batch prediction don\\u2019t have to be mutually exclusive. One hybrid solution is that you precompute predictions for popular queries, then generate predictions online for less popular queries. From Batch Prediction to Online Prediction The more natural way to serve predictions is probably online. You give your model an input and it generates a prediction as soon as it receives that input. You export your model, upload the exported model to Amazon SageMaker or Google App Engine, and get back an exposed endpoint. Now, if you send a request that contains an input to that endpoint, it will send back a prediction generated on that input.A problem with online prediction is that your model might take too long to generate predictions. Instead of generating predictions as soon as they arrive, what if you compute predictions in advance and store them in your database, and fetch them when requests arrive? This is exactly what batch prediction does. Batch prediction can also be seen as a trick to reduce the inference latency of more complex models\\u2014the time it takes to retrieve a prediction is usually less than the time it takes to generate it. Batch prediction is good for when you want to generate a lot of predictions and don\\u2019t need the results immediately. The problem with batch prediction is that it makes your model less responsive to users\\u2019 change preferences. Batch prediction is a workaround for when online prediction isn\\u2019t cheap enough or isn\\u2019t fast enough. Overcome the latency challenge of online prediction:A (near) real-time pipeline that can work with incoming data, extract streaming features (if needed), input them into a model, and return a prediction in near real-time. A streaming pipeline with real-time transport and a stream computation engine can help with that. A model that can generate predictions at a speed acceptable to its end users. For most consumer apps, this means milliseconds.\\u2013\\u2013\\u2013Having two different pipelines to process your data is a common cause for bugs in ML production. One cause for bugs is when the changes in one pipeline aren\\u2019t correctly replicated in the other, leading to two pipelines extracting two different sets of features. This is especially common if the two pipelines are maintained by two different teams.Building infrastructure to unify stream processing and batch processing has become a popular topic in recent years for the ML community. \\u2014> using a stream processor like Apache Flink Model Compression If the model you want to deploy takes too long to generate predictions:make it do inference faster (= inference optimization)make the model smaller (= model compression)make the hardware it\\u2019s deployed on run faster \\u2014> Cheng et al.\\u2019s \\u201cSurvey of Model Compression and Acceleration for Deep Neural Networks\\u201cLow-Rank Factorization = replace high-dimensional tensors with lower-dimensional tensors One type of low-rank factorization is compact convolutional filters, where the over-parameterized (having too many parameters) convolution filters are replaced with compact blocks to both reduce the number of parameters and increase speed. Knowledge Distillation = a small model (student) is trained to mimic a larger model or ensemble of models (teacher) One example of a distilled network used in production is DistilBERT, which reduces the size of a BERT model by 40% while retaining 97% of its language understanding capabilities and being 60% faster advantage: it can work regardless of the architectural differences between the teacher and the student networks For example, you can get a random forest as the student and a transformer as the teacherdisadvantage: it\\u2019s highly dependent on the availability of a teacher network. Pruning 1= remove entire nodes of a neural network, which means changing its architecture and reducing its number of parameters 2= find parameters least useful to predictions and set them to 0 The architecture of the neural network remains the same.\"\n",
      "  },\n",
      "  {\n",
      "    \"id\": 21,\n",
      "    \"chunk\": \"This helps with reducing the size of a model because pruning makes a neural network more sparse, and sparse architecture tends to require less storage space than dense structure. Quantization = reduces a model\\u2019s size by using fewer bits to represent its parameters. Also improves the computation speed. First, it allows us to increase our batch size. Second, less precision speeds up computation, which further reduces training time and inference latency. ML on the Cloud and on the Edge On the cloud means a large chunk of computation is done on the cloud, either public clouds or private clouds. On the edge means a large chunk of computation is done on consumer devices\\u2014such as browsers, phones, laptops, smartwatches, cars, security cameras, robots, embedded devices, FPGAs (field programmable gate arrays), and ASICs (application-specific integrated circuits)\\u2014which are also known as edge devices. easiest way: deploy via a managed cloud service (AWS, GCP)downside: cost - ML models can be compute-intensive, and computing is expensive. https://blog.tomilkieway.com/72k-1/Putting your models on the edge is also appealing when handling sensitive user data. Edge computing makes it easier to comply with regulations, like GDPR, about how user data can be transferred or stored.Because of the many benefits that edge computing has over cloud computing, companies are in a race to develop edge devices optimized for different ML usecases. Compiling and Optimizing Models for Edge Devices For a model built with a certain framework, such as TensorFlow or PyTorch, to run on a hardware backend, that framework has to be supported by the hardware vendor. IRs (intermediate representations) lie at the core of how compilers work. From the original code for a model, compilers generate a series of high- and low-level IRs before generating the code native to a hardware backend so that it can run on that hardware backend. This process is also called lowering, as in you \\u201clower\\u201d your high-level framework code into low-level hardware-native code. It\\u2019s not translating because there\\u2019s no one-to-one mapping between them. Model Optimization A typical ML workflow consists of many frameworks and libraries. For example, you might use pandas/dask/ray to extract features from your data. You might use NumPy to perform vectorization. You might use a pre-trained model like Hugging Face\\u2019s Transformers to generate features, then make predictions using an ensemble of models built with various frameworks like sklearn, TensorFlow, or LightGBM. In many companies, what usually happens is that data scientists and ML engineers develop models that seem to be working fine in development. However, when these models are deployed, they turn out to be too slow, so their companies hire optimization engineers to optimize their models for the1.2.3.4.hardware their models run on. There are two ways to optimize your ML models: locally and globally. Locally is when you optimize an operator or a set of operators of your model. Globally is when you optimize the entire computation graph end to end. Four common techniques for standard local optimization:VectorizationParallelizationLoop tilingOperator fusionUsing ML to optimize ML models There are a couple of drawbacks to hand-designed heuristics. First, they\\u2019re nonoptimal.\"\n",
      "  },\n",
      "  {\n",
      "    \"id\": 22,\n",
      "    \"chunk\": \"There\\u2019s no guarantee that the heuristics an engineer comes up with are the best possible solution. Second, they are nonadaptive. Repeating the process on a new framework or a new hardware architecture requires an enormous amount of effort. If you don\\u2019t have ideas for good heuristics, one possible solution might be to try all possible ways to execute a computation graph, record the time they need to run, then pick the best one. ML in Browsers WASM (WebAssembly) is an open standard that allows you to run executable programs in browsers. After you\\u2019ve built your models in scikit-learn, PyTorch, TensorFlow, or whatever frameworks you\\u2019ve used, instead of compiling your models to run on specific hardware, you can compile your model to WASM. You get back an executable file that you can just use with JavaScript. main drawback of WASM is that because WASM runs in browsers, it\\u2019s slow.\"\n",
      "  }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "json_data = []\n",
    "\n",
    "for i, doc in enumerate(docs):\n",
    "    json_obj = {\n",
    "        \"id\": i,\n",
    "        \"chunk\": doc.page_content\n",
    "    }\n",
    "    json_data.append(json_obj)\n",
    "\n",
    "json_string = json.dumps(json_data, indent=2)\n",
    "\n",
    "# Print the JSON string\n",
    "print(json_string)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
